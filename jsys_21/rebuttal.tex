\documentclass[letterpaper,twocolumn,10pt]{article}
\usepackage{jsys}

% Disable a warning related to \nonfrenchspacing
\microtypecontext{spacing=nonfrench}

\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{bpaxos}
\usepackage{enumitem}
\usepackage{pervasives}
\usepackage{subcaption}
\usepackage{tikz}
\usetikzlibrary{backgrounds}
\usetikzlibrary{calc}
\usetikzlibrary{positioning}

% Reviewer quotes.
\definecolor{ReviewerDarkGray}{HTML}{37474F}
\newenvironment{reviewerquote}
{\list{}{\leftmargin=\parindent\rightmargin=0in}\item[] \itshape \color{ReviewerDarkGray}}%
{\endlist}

\newtoggle{showfeedback}
% \toggletrue{showfeedback}
\togglefalse{showfeedback}
\NewEnviron{feedback}{
  \iftoggle{showfeedback}{
    \begin{itemize}[leftmargin=*]
      \item[] \itshape \textcolor{ReviewerDarkGray}{\BODY}
    \end{itemize}
  }{}
}

\begin{document}
\title{Response to Reviewer Comments}

\maketitle

%% Disable header and footer on the from page.
\thispagestyle{empty}

We'd first like to thank the reviewers for all of their feedback. Below, we've
included our response to the reviewer feedback and our revised paper. Changes
to the paper are shown in \markrevisions{pink}.
% The major changes include...

\section*{Reviewer 1}
\begin{reviewerquote}
  Weakness

  Although the paper targets to be a pedagogical paper, it is not easy to read.
  Cleaning up writing may be necessary.  Paper often gets not precise. Clear
  assumptions and thorough correctness arguments are missing.  I think this
  paper has the potential to be a good tutorial to dependency graph-based
  consensus protocols. (I might change the title. I think this paper is
  specifically on dependency graph-based Paxos protocols, not for general
  multi-leader consensus protocols.) However, it wasn't a paper that reads
  smoothly, especially since this paper presents many different protocols. As
  it is targetting to be a pedagogical paper, I recommend authors to spend time
  on that. In the detailed comments below, I added a few suggestions.

  Also, the paper gets loose on preciseness from time to time. I understand
  that it might be impossible to present proofs to all protocols presented. But
  it sometimes felt too loose to wrap my mind about correctness.

  Finally, I felt the paper wanders among various PPaxos \& other prior work. I
  wonder if setting up a goal upfront would be helpful. It might be useful to
  set the goal of understanding EPaxos like protocols and why people want them.
\end{reviewerquote}

\begin{reviewerquote}
  Page 1.

  \begin{itemize}
    \item
      ``generalized [13, 18]'' $\Rightarrow$ Is it a widely accepted term?
      Personally, I didn't really see that term outside of generalized Paxos.
  \end{itemize}
\end{reviewerquote}

Atlas~\cite{enes2020state}, Caesar~\cite{arun2017speeding}, and a handful of
other papers in the space~\cite{sutra2011fast, losa2016brief} use the term
``generalized'', so we decided to use it as well to be consistent with these
papers.

\begin{reviewerquote}
  \begin{itemize}
    \item
      In the introduction, I would clarify that this paper's scope is on
      dependency graph-based Paxos protocols. It
  \end{itemize}
\end{reviewerquote}

We added a sentence to the introduction clarifying that the paper focuses on
protocols based on dependency graphs.

\begin{reviewerquote}
  Page 2.

  \begin{itemize}
    \item
      ``For example, with two proposers... the leader of the round'' seems not
      important.
  \end{itemize}
\end{reviewerquote}

We removed the example from the paper. We left the sentence clarifying our
usage of the word ``leader'' though, as we use this a lot throughout the paper.

\begin{reviewerquote}
  \begin{itemize}
    \item
      ``chosen'' at the end is not defined. It seems different from ``chosen''
      in page 4. I would clarify ``chosen'' and ``voted'' in page 3.
  \end{itemize}
\end{reviewerquote}

We edited Section 2.2 to make the definition of chosen more clear. Formally, a
value is considered chosen when a majority of the acceptors vote for the value
in the same round.

\begin{reviewerquote}
  Page 3.

  \begin{itemize}
    \item
      Figure 1 added little value to me. Except ``number of leaders'', terms in
      other boxes are not yet defined. Also, it covers many protocols not
      discussed in the paper.
  \end{itemize}
\end{reviewerquote}

We moved Figure 1 to Section 10 at the end of the paper; it's now Figure 15.
Now, all of the terms in the figure are defined when the figure is introduced.

\begin{reviewerquote}
  \begin{itemize}
    \item
      Ditto for Table 1. ``$\maj{}$'' is not defined either.
  \end{itemize}
\end{reviewerquote}

We also moved Table 1 to Section 10 at the end of the paper; it's now Table 2.
As with Figure 1, all of the terms in the table are now defined when the table
is introduced.

\begin{reviewerquote}
  \begin{itemize}
    \item
      I add details to Figure 3. The current version only shows how many
      message delays are required. I think a figure with terms $(vr, vv,
      \PhaseIA{i}, \text{etc})$ used in the text will be very helpful.
  \end{itemize}
\end{reviewerquote}

We added the terms you requested to the figure.

\begin{reviewerquote}
  \begin{itemize}
    \item
      Meaning of ``$v$'' is mixed. Sometimes ``$v$'' means the value given by
      the client. Sometimes it's not.
  \end{itemize}
\end{reviewerquote}

When we discuss state machine replication protocols like MultiPaxos, we try to
use variables like $x$, $y$, and $z$ to denote state machine commands proposed
by the client. When we discuss consensus protocols like Paxos, however, there
is no state machine, so there are no state machine commands. In this context,
we use $v$ to denote values being chosen to be consistent with the existing
literature. You're right that sometimes these values are values proposed by the
client (as in Paxos), and sometimes they are more complicated (like $(x,
\deps{v_x})$ in \BPaxos{}). We tried our best to name variables as consistently
and intuitively as possible, and we tried out a couple of different naming
schemes before settling on the one in the paper. We do apologize that it is
still confusing at times.

\begin{reviewerquote}
  Page 4.

  \begin{itemize}
    \item
      If any message gets ignored, what does a proposer do?
  \end{itemize}
\end{reviewerquote}

We added some text to the end of Section 2.2 clarifying what a proposer does if
it does not receive sufficiently many responses from the acceptors. This can
happen because the acceptors ignored the proposer's requests, because the
network dropped the messages, because of acceptor failure, etc.

\begin{reviewerquote}
  \begin{itemize}
    \item
      Can you elaborate on how replicas work? Why is it safe, and how to ensure
      consistency?
  \end{itemize}
\end{reviewerquote}

Unlike proposers and acceptors, replicas are very passive and do not execute
any complex protocol logic. Replicas simply receive chosen commands from the
proposers, place the commands in their logs, and execute the commands in log
order. This is illustrated in Figure 1. Intuitively, this provides
linearizability almost trivially since every replica is literally executing
every command serially. For a more formal proof of correctness, we refer you to
Lamport's original descriptions of the protocols~\cite{lamport2001paxos,
lamport1998part}.

\begin{reviewerquote}
  Page 5.

  \begin{itemize}
    \item
      Need an explanation on directed edges. There is no introduction of the
      meaning of direction. The definition of dependency explained so far
      doesn't pose any order.
  \end{itemize}
\end{reviewerquote}

We added some text to section 3.1 clarifying the direction of edges as well as
their meaning.

\begin{reviewerquote}
  \begin{itemize}
    \item
      3.2, ``Replicas execute conflict graphs in reverse topological order''
      $\Rightarrow$ why? what goes wrong if executed in topological order? I
      could imagine it's like EPaxos, but unclear. I wish to see some
      explanation based on the ultimate goal.
  \end{itemize}
\end{reviewerquote}

The direction of edges is arbitrary. With minor protocol modifications, we
could flip the direction of the edges and instead execute in topological order.
We adopted EPaxos' convention of reverse topological order to be consistent
with the existing literature, and we believe this convention is more intuitive.

\begin{reviewerquote}
  \begin{itemize}
    \item
      3.3 ``a log one entry'' $\Rightarrow$ typo?
  \end{itemize}
\end{reviewerquote}

Yes, oops. Fixed!

\begin{reviewerquote}
  Page 7.

  \begin{itemize}
    \item
      ``$v_x = (p_i, m)$'' $\Rightarrow$ $v_x$, whose vertex id is $(p_i, m)$.
      Mixing command, vertex, and vertex id was a bit confusing. For example,
      ``globally unique vertex id $v_x = (p_i, m)$'' but later in text, $v_x$
      means vertex, not id.
  \end{itemize}
\end{reviewerquote}

You're right that a vertex and a vertex id are not exactly the same, but
because every vertex is labelled with a globally unique vertex id, we sometimes
refer to the vertex with vertex id $v_x$ as $v_x$ for brevity. The distinction
between commands and vertices is more fundamental. The same command can be
proposed multiple times and may appear in multiple vertices, so we are forced
to distinguish between the two. A vertex $v_x = (p_i, m)$ being constructed of
a proposer id and proposer-local monotonically increasing id is just an
implementation detail to ensure globally unique ids. Any mechanism to construct
globally unique ids can be used.

\begin{reviewerquote}
  Page 8.

  \begin{itemize}
    \item
      Texts for figure 11 are difficult to read. Maybe move them to caption.
  \end{itemize}
\end{reviewerquote}

We increased the font size on the subcaptions.

\begin{reviewerquote}
  \begin{itemize}
    \item
      Here, you introduce the timer. I think Paxos protocols generally don't
      require a synchronous network. I think this new requirement should be
      declared earlier.
  \end{itemize}
\end{reviewerquote}

As we state in Section 2, we don't assume a synchronous network, and we assume
machines can operate at arbitrary speeds without any form of clock
synchronization. The timers used during recovery are local to a machine. They
can run arbitrarily fast or slow without violating the protocol's safety. The
timers do not depend on a synchronous network.

\begin{reviewerquote}
  \begin{itemize}
    \item
      Separate client request (command) from vertex id. If not, it seems that
      clients won't be able to retry after the vertex gets noop.
  \end{itemize}
\end{reviewerquote}

We added some text to Section 4.4 explaining that clients re-propose their
commands if they have not received a response for a sufficiently long period of
time.

\begin{reviewerquote}
  Page 9.

  \begin{itemize}
    \item
      4.5 doesn't prove that the two invariants guarantee linearizability or
      something similar. I would like to see the correctness argument for SMR,
      not just to the two invariants.
  \end{itemize}
\end{reviewerquote}

We added some text to Section 3.4 explaining why the two key invariants are
sufficient to guarantee linearizable execution.

\begin{reviewerquote}
  \begin{itemize}
    \item
      ``commit time'' is typically defined as the time period between a client
      sends a request and the client knows it's committed. Execution is not a
      requirement. It's an important distinction for EPaxos, whose commit
      latency is different from execution latency (which may be longer than
      commit latency).
  \end{itemize}
\end{reviewerquote}

This is a subtle point. If we define commit time as the period of time between
when a client proposes a command when the client learns the value is committed,
then we can define ``committed'' in a cheeky way to artificially lower the
commit time to its theoretical lower bound.

For example, consider the following hypothetical MultiPaxos variant. To propose
a command, a client first broadcasts the command to $2f+1$ storage nodes. When
a storage node receives a command, it stores the command and replies back to
the client. When a client receives replies from $f+1$ storage nodes, it
considers the command ``committed''. Despite $f$ failures, the command will be
available on some storage node. Some other daemon process that runs in the
background periodically scans the storage nodes and proposes any commands it
finds to a MultiPaxos instance. Thus, when a command is ``committed'', it will
eventually be chosen and executed. This protocol achieves the theoretical
lowest commit time of two network delays, but it is worse than executing
MultiPaxos directly.

We think that defining commit time with respect to executing a command, rather
than ``committing'' it, is less ambiguous and more intuitive. Through personal
conversations with the EPaxos authors, they have also come to regret separating
command commitment from execution, as it complicates the protocol for marginal
gain.

\begin{reviewerquote}
  \begin{itemize}
    \item
      I don't understand how EPaxos leverages FastPaxos. Also, EPaxos's key
      advantage is optimistic two wide-area message delays. More explanation is
      needed here on the delicate need of FastPaxos \& why 4 message delays are
      considered ``improvement'' despite MultiPaxos also only needs 4 message
      delays. I guess adding multi-leader requires more message delays, which
      could be removed by ``FastPaxos''.
  \end{itemize}
\end{reviewerquote}

MultiPaxos has four network delays in the best case because the client has to
send the command to the leader (1), the leader has to perform a round trip of
communication with the acceptors (2, 3), and the leader has to reply to the
client (4). Simple \BPaxos{} requires an additional round trip to the
dependency service because the protocol is generalized. Unanimous \BPaxos{},
EPaxos, and Atlas all leverage Fast Paxos to eliminate this extra round trip.
Specifically, it uses Fast Paxos to merge the round trip with the dependency
service with the round trip to the acceptor.  You're 100\% right that if we
look purely at commit time, these protocols are just as good as MultiPaxos.
Though note that these protocols can be optimized to remove an additional
network delay (MultiPaxos can't do that), but it complicates the protocols.

\begin{reviewerquote}
  Page 10.

  \begin{itemize}
    \item
      Why use both learner and replica? Any difference?
  \end{itemize}
\end{reviewerquote}

Consensus protocols like Paxos and Fast Paxos don't replicate a state machine,
so they don't have state machine replicas. In this context, we use the term
learner. State machine replication protocols like MultiPaxos, on the other
hand, have state machine replicas which act as learners. In this context, we
call them replicas.

\begin{reviewerquote}
  \begin{itemize}
    \item
      I feel ``$\maj{}$'' is awkward. Why not just use an equation? Also, I
      don't understand why $\maj{}$ needs to be defined separately for odd $n$
      and even $n$.
  \end{itemize}
\end{reviewerquote}

We edited the definition of $\maj{}$ in Section 5 to be $\maj{n} =
\ceil{\frac{n+1}{2}}$. We think $\maj{}$ is more intuitive and less error prone
than reading raw equations. For example, we prefer to read $\maj{f+1}$ and
$\maj{f+1} - 1$ than $\ceil{\frac{f+2}{2}}$, $\floor{\frac{f+1}{2}} + 1$,
$\ceil{\frac{f+1}{2}}$, and $\floor{\frac{f+1}{2}}$.

\begin{reviewerquote}
  \begin{itemize}
    \item
      If you used the commit time defined earlier, it can get more than 4. It
      must wait for dependent commands.
  \end{itemize}
\end{reviewerquote}

Great point. We edited Section 5 to mention that commit time refers to the best
case. In general due to the FLP impossibility
result~\cite{fischer1982impossibility}, a command can always get delayed
arbitrarily for any state machine replication protocol.

\begin{reviewerquote}
  \begin{itemize}
    \item
      6.2 Recovery $\Rightarrow$ the term recovery is overloaded. Here, it's
      just adding noop. In other places, it commits with real command.
  \end{itemize}
\end{reviewerquote}

You're 100\% right. A Simple \BPaxos{} proposer can use the same procedure to
try to get a noop chosen or to try to get any other value chosen. The recovery
procedure only involves a proposer executing a full round of Fast Paxos. This
is an advantage of Simple \BPaxos{} compared to other protocols that have an
entirely separate and more complex recovery procedure.

There's also something subtle here. When a Paxos proposer executes Phase 1 as
part of a recovery, it doesn't know ahead of time which value it will choose.
If after Phase 1, the proposer learns that some value may have already been
chosen in a previous round, it is forced to propose it. Only if the proposer
learns that no value has been chosen in a previous round, can the proposer
propose any value (e.g., a noop).

\begin{reviewerquote}
  \begin{itemize}
    \item
      In the bullet starting with ``In Figure 14b,'' $\Rightarrow$ remind
      readers about Paxos protocol. I think that's necessary to understand why
      ``so is forced to propose $(x, p_i)$''.
  \end{itemize}
\end{reviewerquote}

We added some text to Section 6.3 reminding the reader about Paxos.

\begin{reviewerquote}
  \begin{itemize}
    \item
      I initially didn't understand why proposer $p_2$ doesn't send the request
      to $d_1$, $d_2$, $\ldots$. It felt more natural to do that. But I guess
      you strictly abided by the Paxos protocol.
  \end{itemize}
\end{reviewerquote}

You're right. Proposers recover vertices by executing Paxos, so there is no
need to communicate with the dependency service.

\begin{reviewerquote}
  Page 14.

  \begin{itemize}
    \item
      7.1 $\Rightarrow$ first paragraph is complicated. Renaming phases and
      describing what they do would be helpful.
  \end{itemize}
\end{reviewerquote}

We agree that the Phase 1 and Phase 2 terminology is a little opaque, but we
use the terminology to be consistent with the existing
literature~\cite{howard2021fast}.

\begin{reviewerquote}
  Page 15.

  \begin{itemize}
    \item
      Does Unanimous PPaxos modify the ``Paxos'' protocol part? Can you explain
      why replicas will still converge and agree?
  \end{itemize}
\end{reviewerquote}

As we state in Section 7.1, Unanimous \BPaxos{} is identical to Fast \BPaxos{}
except with fast Phase 2 quorums of size $2f+1$. Unanimous \BPaxos{} proposer
pseudocode is given in Algorithm 3. It is identical to the pseudocode of a Fast
Paxos proposer (Algorithm 1) except for a couple small changes highlighted in
red.

\section*{Reviewer 2}
\begin{reviewerquote}
  Measurable benefits of switching to multi-leader protocols

  Multi-leader consensus protocols are complex and it's harder to implement
  them than their leadered counterparts but in theory they have better
  performance. As a practitioner I wonder if the benefits are worth the
  complexity but the paper doesn't give me an answer.

  The authors advocate that the major benefit comes as increased throughput (in
  Raft/Paxos a leader sends and receives disproportionately more messages). I
  recommend to include either experiment data or an estimation of the upper
  limits of throughput for both systems.

  E.g. napkin math shows that the throughput of multi-leader consensus is
  $(2f+1)^2/(4f+1)$ times better than leadered where $f$ is max allowed numbers
  of failures, proposer, acceptors and learners are colocated, it uses $f+1$
  sized quorums and the command's payload is way more bigger than the result of
  command execution.
\end{reviewerquote}

We wholeheartedly agree that a comprehensive evaluation of the protocols would
greatly improve the paper. Unfortunately, we believe that such a performance
comparison falls outside of the scope of our paper and would likely be such a
large contribution that it would warrant a paper on its own.

We also believe that our paper will enable such a contribution in the future.
Implementing and evaluating a generalized multi-leader state machine protocol
is extremely challenging without first understanding the protocol deeply, and
currently, there is no easy way to understand these protocols. Our hope is that
our paper will allow researchers to understand these protocols at a much deeper
level, paving the way for future research on performance comparisons.

\begin{reviewerquote}
  Retrieving data from the state machines

  Usually production Paxos-based systems depend on bypassing the replication
  protocol for serving reads but the paper doesn't contain any information on
  how to do it. For example the leader-based protocols have well understood
  models for lightweight reads:

  to read from a leader after waiting for a round of the heartbeats or after
  checking that its lease doesn't expire to read from a combination of the
  followers using methods from ``Linearizable Quorum Reads in Paxos'' or
  ``Paxos quorum leases: Fast reads without sacrificing writes'' papers to
  accept a possibility of staleness and to read from any node But those models
  aren't applicable to the multi-leader case: it lacks the leader/follower
  roles and local reads lead not only to staleness but to observing
  incompatible histories too caused by the reordering of the commuting
  commands.
\end{reviewerquote}

We agree that optimizing reads is important for practical deployments of state
machine replication protocols, but we believe that inventing a novel
optimization to speed up reads in generalized multi-leader protocols would
warrant its own research paper and extends beyond the scope of our
Systemization of Knowledge paper which is targeted at clarifying the existing
body of literature on generalized multi-leader state machine replication
protocols. We hope that our paper will enable researchers to invent novel read
optimizations in the future.

\begin{reviewerquote}
  Reconfiguration

  The last important missing part is reconfiguration. It's crucial from a
  practical perspective. Without reconfiguration there is no way to replace the
  failed nodes and since failures are inevitable then eventually the
  multi-leader consensus based systems lose a majority and become unavailable.

  One of the reasons Raft got so widespread in the industry is an included
  reconfiguration protocol (joined consensus) so with the declared goal I
  recommend to update the paper to include the reconfiguration sub-protocol.
\end{reviewerquote}

We totally agree that reconfiguration is an essential component of any state
machine replication protocol, and without it, a protocol is unusable in
practice. We have also submitted a paper titled ``Donut Paxos: A Reconfigurable
Consensus Protocol'' to JSys that presents a novel reconfiguration protocol
that can be adopted by generalized multi-leader state machine replication
protocols more easily compared to existing approaches like MultiPaxos'
horizontal reconfiguration or Raft's joint consensus. We believe this piece of
research stands best as its own separate paper.

\begin{reviewerquote}
  Other (minor) issues

  ``This paper assumes that at most $f$ machines will fail'' - from a practical
  standpoint it's impossible to guarantee; it's better to mention what happens
  then this threshold is passed if consistency or availability becomes
  violated.
\end{reviewerquote}

We added some text to Section 2 explaining that the assumption of at most $f$
failures is made only for liveness. All the protocols in the paper are safe
despite any number of failures (except for Fast \BPaxos{}, which is
intentionally unsafe).

\begin{reviewerquote}
  ``To reach is consensus on a value, an execution of Paxos is divided into a
  number of integer value rounds'' - it's a clever idea to pre-assign a set of
  ballot numbers to the machines but using a term ``round'' both for a ballot
  number and an act of communication is confusing. It may create a false
  impression that it's impossible to skip ballot numbers. Maybe it'd be better
  to mention that rounds also known as ballot numbers (paxos) or term (raft)
  and add a reference to ``Paxos vs Raft: Have we reached consensus on
  distributed consensus?'' to use it as a map between different protocols.
\end{reviewerquote}

Great idea. We edited Section 2.2 with your suggestion.

\section*{Reviewer 3}
\begin{reviewerquote}
  Cons:

  \begin{enumerate}
    \setcounter{enumi}{0}
    \item
      I strongly believe that this paper is missing some evaluation. It will be
      really helpful if the authors can illustrate the performance of the four
      protocols they have proposed against some of the protocols like Epaox,
      Atlas and so on. Although the aim of this paper is not to evaluate
      performance of multi-leader protocols, graphs that illustrate the
      performance of the proposed variants can help understand various design
      choices, such as which scheme is better Tension Avoidance or Tension
      Resolution.
  \end{enumerate}
\end{reviewerquote}

We wholeheartedly agree that a comprehensive evaluation of the protocols would
greatly improve the paper. Unfortunately, we believe that such a performance
comparison falls outside of the scope of our paper and would likely be such a
large contribution that it would warrant a paper on its own.

We also believe that our paper will enable such a contribution in the future.
Implementing and evaluating a generalized multi-leader state machine protocol
is extremely challenging without first understanding the protocol deeply, and
currently, there is no easy way to understand these protocols. Our hope is that
our paper will allow researchers to understand these protocols at a much deeper
level, paving the way for future research on performance comparisons.

\begin{reviewerquote}
  \begin{enumerate}
    \setcounter{enumi}{1}
    \item
      This paper needs some more work to clarify the different proposed
      protocols. Further, for several sections, there is a sudden switch from
      normal phase to recovery phase, which makes it difficult to understand. I
      explain most of my concerns next:
  \end{enumerate}
\end{reviewerquote}

We addressed all of your concerns below.

\begin{reviewerquote}
  Major comments:

  \begin{enumerate}
    \setcounter{enumi}{0}
    \item
      While explaining the recovery for Simple PPaxos, the paper mentions
      ``unchosen vertex'' at the start of Section 4.4. It will be helpful if
      the paper explains what is meant by unchosen here. Further, it is
      important to re-iterate how can a vertex be unchosen.
  \end{enumerate}
\end{reviewerquote}

Great point. We added some text to the beginning of Section 4.4 to explain what
an unchosen vertex is and how a vertex can become unchosen.

\begin{reviewerquote}
  \begin{enumerate}
    \setcounter{enumi}{1}
    \item
      I am unsure why at line 21 of Algorithm 1, the propose is sending to at
      least $f+1$ acceptors instead of all acceptors. As $f$ of the acceptors
      may crash, so sending to all acceptors in important.
  \end{enumerate}
\end{reviewerquote}

You're right. We were implementing an optimization called
``thriftiness''~\cite{moraru2013there} or ``cheapness''~\cite{lamport2004cheap}
in which proposers send to the bare minimum number of acceptors, but we didn't
mentioned this in the paper and it's is more complicated than necessary. We
replaced all mention of ``at least $f+1$'' acceptors to just ``the acceptors''
throughout the paper.

\begin{reviewerquote}
  \begin{enumerate}
    \setcounter{enumi}{2}
    \item
      It is unclear to me what is the benefit of waiting for messages from all
      the $2f+1$ acceptors in line 1, Algorithm 3. The paper should explain what
      will be the issue if the proposer only waits for messages from $f +
      \maj{f+1}$ acceptors. I understand that the change at line helps to resolve
      the safety bug of Fast PPaxos but not sure why change at line 1
      important. Moreover, asking Unanimous Paxos's proposer to wait for
      messages from all the $2f+1$ acceptors can hurt its liveness even if a
      single proposer fails. In such a case, how will the proposer recover the
      state. The paper needs to clarify this scenario.
  \end{enumerate}
\end{reviewerquote}

We revised Section 7.1 to reference line 1 of Algorithm 3. The main idea behind
increasing Phase 2 quorums to size $2f+1$ on line 1 is to ensure that on line
18, a proposer is only forced to propose a set of dependencies if that set of
dependencies was computed by $f+1$ dependency service nodes. This allows the
protocol to maintain the consensus and dependency invariants.

We also added a paragraph to the end of Section 7.1 discussing the
disadvantages of having unanimous quorums, including, as you mention, the
decreased liveness.

\begin{reviewerquote}
  \begin{enumerate}
    \setcounter{enumi}{3}
    \item
      I think there is a typo for lines 6-7 in Algorithm 3. They are not
      identical to the lines is Algorithm 1.
  \end{enumerate}
\end{reviewerquote}

Good catch. Those lines are different but not a typo. We edited Algorithm 3 to
highlight them in red, along with the other changes made to Algorithm 1. Those
lines are actually a simplification to the more generic Fast Paxos proposer
pseudocode of Algorithm 1. Because Phase 2 quorums are unanimous in Unanimous
\BPaxos{}, if the proposer of round $0$ does not receive a unanimous set of
votes in round $0$, it knows immediately that no value has been or ever will be
chosen in round $0$, so it can proceed immediately to Phase 2 of round $1$ and
propose that any value be chosen.

\begin{reviewerquote}
  \begin{enumerate}
    \setcounter{enumi}{4}
    \item
      While explaining the Basic Epaxos optimization on Step 7, page 16, the
      paper states that ``pi receives $2f$ votes for... including a vote from
      $d_i$''.  I am not sure when did $d_i$ sent a vote to $p_i$. In the
      presented algorithm, $d_i$ only communicates with other $d_j$. This step
      needs to be clarified.
  \end{enumerate}
\end{reviewerquote}

Oops! That's a typo. $d_i$ should be $a_i$. We fixed that in the paper. The
idea is that a proposer always receives a vote from its colocated acceptor
(which was proposed by the colocated dependency service node).

\begin{reviewerquote}
  \begin{enumerate}
    \setcounter{enumi}{5}
    \item
      It is unclear what is happening from steps (7) and (8) on Basic Epaxos.
      So, when $p_i$ receives $2f$ votes it sends $v$ to $a_i$. Then, $a_i$
      records $v$ as chosen and sends back to $p_i$. Then $p_i$ sends to all
      other ancestors, and waits for acknowledgments. Why can't these steps be
      combined? Further, to reduce from $2f+1$ to $2f$, the Basic Epaxos
      optimization requires more network communication--from proposer to all
      acceptors and then acknowledgments from acceptors to the proposer. This
      needs to be clarified.
  \end{enumerate}
\end{reviewerquote}

You're right that when $p_i$ receives $2f$ votes it sends $v$ to $a_i$. Then,
$a_i$ records $v$ as chosen and sends back to $p_i$. After that, however, $p_i$
does not have to communicate with the acceptors. It notifies the replicas that
$v$ has been chosen. These messages are labeled (8) in Figure 14. Also note
that messages (6) and (7) are local to the server hosting $p_i$, so they don't
incur any network delays. We edited Section 7.2 to clarify this. In practice,
you would likely implement every node on a server within a single process, so
the steps would effectively be combined. We keep all nodes decoupled to keep
the protocols simple.

Simple \BPaxos{} with the Basic EPaxos optimization actually requires the same
or less communication than Simple \BPaxos{} without the optimization. Without
the optimization, the proposer must communicate with all $2f+1$ acceptors. With
the optimization, the proposer can implement thriftiness~\cite{moraru2013there}
and only communicate with $2f$ acceptors, or it can opt out of thriftiness and
communicate with all $2f+1$.

\begin{reviewerquote}
  Minor comments:

  \begin{enumerate}
    \setcounter{enumi}{0}
    \item
      On Page 6, the paper states Consensus Invariants, which uses the terms
      $(x, \deps{x})$. These terms have not been mentioned anytime before in
      this subsection. Although it can be understood by further reading, it
      will be helpful to the reader if explained before.
  \end{enumerate}
\end{reviewerquote}

We edited Section 3.4 to clarify the terms $(x, \deps{v})$ before introducing
the consensus invariant.

\begin{reviewerquote}
  \begin{enumerate}
    \setcounter{enumi}{1}
    \item
      It will be helpful if the paper either uses the term replicas or learners.
      The paper switches between the terms, which makes it often confusing.
  \end{enumerate}
\end{reviewerquote}

Consensus protocols like Paxos and Fast Paxos don't replicate a state machine,
so they don't have state machine replicas. In this context, we use the term
learner. State machine replication protocols like MultiPaxos, on the other
hand, have state machine replicas which act as learners. In this context, we
call them replicas. We apologize for any confusion.

\begin{reviewerquote}
  \begin{enumerate}
    \setcounter{enumi}{2}
    \item
      The following line ``We consider what happens when not every...'' on page
      11, left hand side column are confusing the reader, as I expected that
      following this line the solution will be presented, but that happens much
      later in this section. Either consider removing this line or re-write it a
      little.
    \item
      It is worth separating the recovery algorithm and not receiving enough
      matching votes (in Fast Paxos section) with suitable titles to improve
      readability.
  \end{enumerate}
\end{reviewerquote}

Great suggestions. We added subsections, and used the subsections to clarify
the ``We consider what happens when not every...'' sentence.

\section*{Reviewer 4}
\begin{reviewerquote}
  However, I am not entirely certain what went wrong for Fast PPaxos. The paper
  claims that it is due to a dilemma between reaching consensus and preserving
  dependency. But it seems to me that the bug is a result of multi-leader
  rather than dependency. It seems that the bug could be avoided by simply
  abandoning concurrent leaders. The first approach to fix this bug, increasing
  quorum size to $2f+1$, also does not seem related to dependency; rather, it
  seems to be avoiding a tie between concurrent leaders. I would like to see
  more discussion on the root cause of the problem, and how exactly it is
  fixed. This is very important for the paper since at some point the authors
  claim that most of the design complexity comes from generalized as opposed to
  multi-leader.
\end{reviewerquote}

We don't attribute Fast \BPaxos{}'s lack of safety directly to the fact that it
is multi-leader or generalized. In fact, while the execution in Figure 13 has
multiple leaders, none of these leaders has to execute concurrently. Instead,
we attribute the lack of safety to the fact that Fast \BPaxos{} does not
correctly manage the tension between the consensus and dependency invariants.
Increasing quorum sizes to $2f+1$ resolves this tension. In short, being
multi-leader or being generalized are ways to describe a protocol, but they do
not inherently lead to lack of safety. It is the failure to maintain core
invariants that leads to the safety violations.

\begin{reviewerquote}
  \begin{itemize}
    \item
      When describing Fast Paxos, the authors use proposals, acceptors, and
      learners; but in Simple or Fast PPaxos, the authors use proposals,
      acceptors, and replicas. I suggest the authors stick to one convention,
      preferrably learners.
  \end{itemize}
\end{reviewerquote}

Consensus protocols like Paxos and Fast Paxos don't replicate a state machine,
so they don't have state machine replicas. In this context, we use the term
learner. State machine replication protocols like MultiPaxos, Simple \BPaxos{},
and Fast \BPaxos{}, on the other hand, have state machine replicas which act as
learners. In this context, we call them replicas.

\begin{reviewerquote}
  \begin{itemize}
    \item
      Page 11 right column touched on why the threshold must be > 1.5f. While
      this may be obvious for experts, it is helpful to elaborate on this in a
      tutorial paper for non-expert readers.
  \end{itemize}
\end{reviewerquote}

We added a paragraph to Section 5.2 elaborating on the requirement of fast
Phase 2 quorums to be of size $f + \maj{f+1}$.

\begin{reviewerquote}
  \begin{itemize}
    \item
      Section 2 says that messages can be dropped, and indeed Section 6.3
      considers a situation that messages to d3, d4, d5 are dropped. However,
      the default model of Paxos does not allow message drop. Otherwise, one
      runs into the two-general impossibility. In this particular example in
      Section 6.3, can we simply have p1 crash before it sends messages to d3,
      d4,and d5? If this is sufficient to break Fast PPaxos, then I suggest
      removing message drop from the model. If this is not sufficient, the
      authors need to carefully define the model regarding message drop, and
      explain why the two general's impossibility does not apply.
  \end{itemize}
\end{reviewerquote}

All of the protocols discussed in the paper are safe despite the possibility of
messages being dropped, delayed, or reordered (except for Fast \BPaxos{} which
is intentionally unsafe). On the contrary, due to the FLP impossibility
result~\cite{fischer1982impossibility}, none of the protocols are guaranteed to
be fully live. The two generals impossibility result involves establishing
common knowledge between a set of participants, which is slightly orthogonal to
consensus.

\begin{reviewerquote}
  Overall, I like this tutorial paper and would support its acceptance as a
  tutorial paper. However, it is currently submitted as an SoK paper. If judged
  as an SoK paper, I am less supportive. The paper mentions that there are few
  generalized multi-leader systems, and indeed it only reviews 3 papers in
  detail (EPaxos, Caesar, Atlas). The related work section mentions a few other
  works very briefly and they are not closely related. As such, I feel the area
  of generalized multi-leader state machine replication is still relatively
  young and small, and has not reached a stage of needing an SoK paper. This is
  not a criticism of the paper. I think the authors are well aware that their
  paper is more of a tutorial than SoK. The onus is on the editorial board to
  accept this paper under an appropriate category.
\end{reviewerquote}

We completely agree. We're more than happy to have the paper moved to the most
appropriate category available.

\bibliographystyle{plain}
\bibliography{references}

% \clearpage

\end{document}
