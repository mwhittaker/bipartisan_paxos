\section{Fast \BPaxos{}}\seclabel{UnsafeBPaxos}
Simple \BPaxos{} is designed to be easy to understand, but as shown in
\figref{SimpleBPaxosExample}, it takes seven network delays (in the best case)
between when a client proposes a command $x$ and when a client receives the
result of executing $x$. Call this duration of time the \defword{commit time}.
Multi-leader generalized protocols like EPaxos, Caesar, and Atlas all achieve a
commit time of only four network delays.
%
In this section, we present a purely pedagogical protocol called \defword{Fast
\BPaxos{}}. Fast \BPaxos{} achieves a commit time of four network delays, but
Fast \BPaxos{} is unsafe. It does not properly implement state machine
replication. Why discuss an unsafe protocol? To understand why more complex
protocols like EPaxos, Caesar, and Atlas work the way they do, it helps to
understand why simpler protocols like Fast \BPaxos{} don't work in the first
place. Understanding why Fast \BPaxos{} is unsafe leads to fundamental insights
into these other protocols.

\subsection{The Protocol}
{\input{figures/unsafe_bpaxos_diagram.tex}}

Fast \BPaxos{} is largely identical to Simple \BPaxos{} with one key
observation. Rather than implementing Paxos, Fast \BPaxos{} implements Fast
Paxos. This allows the protocol to reduce the commit time by overlapping
communication with the dependency service (to compute dependencies) and
communication with the acceptors (to implement consensus).

As shown in \figref{FastBPaxos}, a Fast \BPaxos{} deployment consists of $2f+1$
dependency service nodes, $2f+1$ Fast Paxos acceptors, $2f+1$ Fast Paxos
proposers, and $2f+1$ replicas. These logical nodes are co-located on a set of
$2f+1$ servers, where every physical server executes a one dependency service
node, one acceptor, one proposer, and one replica. For example, in
\figref{FastBPaxos}, server 2 executes $d_2$, $a_2$, $p_2$, and $r_2$. As
illustrated in \figref{FastBPaxos}, the protocol executes as follows:

\begin{itemize}
  \item \textbf{(1)}
    When a client wants to propose a state machine command $x$, it sends $x$ to
    \emph{any} of the proposers.

  \item \textbf{(2)}
    When a proposer $p_i$ receives a command $x$, from a client, it places $x$
    in a vertex with globally unique vertex id $v_x = (p_i, m)$ where $m$ is a
    monotonically increasing integer local to $p_i$. $p_i$ then sends $x$ and
    $v_x$ to all of the dependency service nodes. Note that we predetermine
    that proposer $p_i$ is responsible for executing round 0 and round 1 of the
    Fast Paxos instance associated with vertex $v_x = (p_i, m)$.

  \item \textbf{(3)}
    When a dependency service node $d_j$ receives a command $x$ in vertex
    $v_x$, it computes a set of dependencies $\deps{v_x}$ in the exact same way
    as in Simple \BPaxos{} (i.e.\ $d_j$ maintains an acyclic conflict graph).
    Unlike Simple \BPaxos{} however, $d_j$ does not send $\deps{v_x}$ back to
    the proposer. Instead, it proposes to the co-located Fast Paxos acceptor
    $a_j$ that the value $(x, \deps{v_x})$ be chosen in the instance of Fast
    Paxos associated with vertex $v_x$ in round 0.

  \item \textbf{(4)}
    Fast \BPaxos{} acceptors are unmodified Fast Paxos acceptors. In the
    normal case, when an acceptor $a_j$ receives value $(x, \deps{v_x})$ in
    vertex $v_x = (p_i, m)$, it votes for the value and sends the vote to
    $p_i$.

  \item \textbf{(5)}
    Fast \BPaxos{} proposers are unmodified Fast Paxos proposers. In the
    normal case, $p_i$ receives $f + \maj{f+1}$ votes for value $(x,
    \deps{v_x})$ in vertex $v_x$, so $(x, \deps{v_x})$ is chosen. The proposer
    broadcasts the command and dependencies to the replicas.
    %
    If $p_i$ receives $f + \maj{f+1}$ votes, but they are not all for the same
    value, the proposer executes coordinate recovery (see
    \algoref{FastPaxosProposer} \lineref{CoordinatedRecovery1} --
    \lineref{CoordinatedRecovery2}).

  \item \textbf{(6)}
    Fast \BPaxos{} replicas are identical to Simple \BPaxos{} replicas.
    Replicas maintain and execute conflict graphs, returning the results of
    executing commands to the clients.
\end{itemize}

Note that \figref{FastBPaxos} illustrates six steps of execution, but the
commit time is only four network delays (illustrated in red). Communication
between co-located components (e.g., between $d_1$ and $a_1$ and between $p_1$
and $r_1$) does not involve the network and therefore does not contribute to
the commit time.

\subsection{Recovery}
As with Simple \BPaxos{}, it is possible that a command $y$ chosen in vertex
$v_y$ depends on an unchosen vertex $v_x$ that must be recovered for execution
to proceed. Fast \BPaxos{} performs recovery in the same way as Simple
\BPaxos{}. If a replica detects that a vertex $v_x$ has been unchosen for some
time, it notifies a proposer. The proposer then executes a Fast Paxos recovery
to get a $\noop{}$ chosen in vertex $v_x$ with no dependencies.

\subsection{Lack of Safety}
{\input{figures/fast_bpaxos_bug.tex}}

We now demonstrate why Fast \BPaxos{} is unsafe. Consider the execution of Fast
\BPaxos{} ($f = 2$) illustrated in \figref{FastBPaxosBug}.
\begin{itemize}
  \item
    In \figref{FastBPaxosBug1}, proposer $p_1$ receives command $x$ from a
    client. It places $x$ in vertex $v_x$ and sends $x$ and $v_x$ to the
    dependency service. $d_1$ and $d_2$ receive the message. They place $x$ in
    their conflict graphs without any dependencies, and send the value $(x,
    \emptyset{})$ to their co-located acceptors. $a_1$ and $a_2$ vote for $(x,
    \emptyset)$ in vertex $v_x$, but $p_1$ crashes before it receives any of
    these votes. The messages sent to $d_3$, $d_4$, and $d_5$ were dropped by
    the network.

  \item
    Similarly in \figref{FastBPaxosBug1}, proposer $p_5$ receives a conflicting
    command $y$, $p_5$ sends $y$ and $v_y$ to $d_4$ and $d_5$, $d_4$ and $d_5$
    propose $(y, \emptyset{})$  to $a_4$ and $a_5$, $a_4$ and $a_5$ vote for
    the proposed values, and $p_5$ fails.

  \item
    In \figref{FastBPaxosBug2}, $p_2$ recovers vertex $v_x$. To recover $v_x$,
    $p_2$ executes Phase 1 of Fast Paxos with acceptors $a_1$, $a_2$, and
    $a_3$. $p_2$ observes that $a_1$ and $a_2$ both voted for the value $(x,
    \emptyset{})$ in round $0$. Therefore, $p_2$ concludes that $(x,
    \emptyset{})$ may have been chosen in round $0$, so it proceeds to Phase 2
    and gets the value $(x, \emptyset)$ chosen in vertex $v_x$
    (\algoref{FastPaxosProposer} \lineref{majority}). $p_2$ cannot propose any
    other value (e.g., a $\noop$) because $(x, \emptyset{})$ may have already
    been chosen. From our omniscient view of the execution, we know that $(x,
    \emptyset)$ was never chosen, but from $p_2$'s myopic view, it cannot make
    this determination and so is forced to propose $(x, \emptyset{})$. This is
    a \textbf{critical point} in the execution, which we will discuss further
    in a moment.

  \item
    In \figref{FastBPaxosBug2}, proposer $p_4$ recovers vertex $v_y$ in much
    the same way as $p_2$ recovers $v_x$. $p_4$ observes that $a_4$ and $a_5$
    voted for $(y, \emptyset)$ in round 0, so it is forced to get the value
    $(y, \emptyset{})$ chosen.

  \item
    Finally in \figref{FastBPaxosBug2}, proposers $p_2$ and $p_4$ broadcast $(x,
    \emptyset)$ and $(y, \emptyset)$ to all of the replicas. The replicas place
    $x$ and $y$ in their conflict graphs without edges between them. This
    violates the dependency invariant. $x$ and $y$ conflict, so there must be
    an edge between them. Without an edge, the replicas can execute $x$ and $y$
    in different orders, causing their states to diverge.
\end{itemize}

What went wrong? When $p_2$ was recovering $v_x$, Fast Paxos forced it to
choose $(x, \emptyset{})$. However, the dependencies $\deps{v_x} = \emptyset{}$
were \emph{not} computed by a majority of the dependency service nodes. They
were computed only by $d_1$ and $d_2$. This is what allowed conflicting
commands $x$ and $y$ to be chosen without a dependency on each other.

This example illustrates a \textbf{fundamental tension} between preserving the
consensus invariant (\invref{ConsensusInvariant}) and preserving the dependency
(\invref{ConflictInvariant}). Maintaining the consensus invariant in isolation
is easy (e.g., use Paxos), and maintaining the dependency invariant in
isolation is also easy (e.g., use the dependency service). But, maintaining
both invariants simultaneously is tricky.

When performing a recovery, we reach a critical point where consensus forces us
to choose \emph{any} value if it may have already been chosen, while the
dependency service requires that only \emph{certain} values be chosen (i.e.\
those computed by a majority of dependency service nodes).
%
This leads to situations, like the one in our example above, where a proposer
is forced to propose a particular value (e.g., $(x, \emptyset{})$) in order to
properly implement consensus and simultaneously forced \emph{not} to propose
the value in order to correctly compute dependencies.

Resolving the tension between the consensus and dependency invariants during
recovery is the single most important and the single most challenging aspect of
multi-leader generalized protocols like EPaxos, Caesar, and Atlas. All of these
protocols have a similar structure and execution in the normal path. They all
compute dependencies from a majority of servers, and they all execute Fast
Paxos variants to get these dependencies chosen. If you understand the normal
case execution of one of these protocols, it is not difficult to understand the
others. The key feature that distinguishes the protocols is how they resolve
the fundamental tension between implementing consensus and computing
dependencies.

These protocols all take different approaches to resolving the tension. In the
next two sections, we broadly categorize the approaches into two main
techniques: \emph{tension avoidance} and \emph{tension resolution}.  Tension
avoidance involves cleverly manipulating quorum sizes to avoid the tension
entirely. This approach is used by Simple EPaxos and Atlas. The second
technique, tension resolution, is significantly more complicated and involves
detecting and resolving the tension through various means.

% In the next few sections, we introduce a handful of \BPaxos{} protocols, each of
% which runs into this fundamental tension. Every \BPaxos{} protocol must resolve
% the tension between the consensus service and dependency service, forcing the
% two parties to cooperate in order to maintain \invref{ConsensusInvariant} and
% \invref{ConflictInvariant}. The distinguishing feature of every protocol is how
% it resolves the tension.  The cooperation between these two opposing parties is
% why we named our protocols Bipartisan Paxos.
