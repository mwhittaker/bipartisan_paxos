\section{Fast \BPaxos{}}\seclabel{FastBPaxos}
In this section, we present a purely pedagogical protocol called \defword{Fast
\BPaxos{}}. Fast \BPaxos{} achieves a commit time of four network delays
(compared to Simple \BPaxos{}' seven), but Fast \BPaxos{} is unsafe. It does not
properly implement state machine replication. To understand why more complex
protocols like EPaxos, Caesar, and Atlas work the way they do, it helps to
understand why simpler protocols like Fast \BPaxos{} don't work in the first
place. Understanding why Fast \BPaxos{} is unsafe leads to fundamental insights
into these other protocols.

\subsection{The Protocol}
{\input{figures/fast_bpaxos.tex}}

Fast \BPaxos{} is largely identical to Simple \BPaxos{} with one key
observation. Rather than implementing Paxos, Fast \BPaxos{} implements Fast
Paxos. This allows the protocol to reduce the commit time by overlapping
communication with the dependency service (to compute dependencies) and
communication with the acceptors (to implement consensus).

As shown in \figref{FastBPaxos}, a Fast \BPaxos{} deployment consists of $2f+1$
dependency service nodes, $2f+1$ Fast Paxos acceptors, $2f+1$ Fast Paxos
proposers, and $2f+1$ replicas. These logical nodes are co-located on a set of
$2f+1$ servers, where every physical server executes one dependency service
node, one acceptor, one proposer, and one replica. For example, in
\figref{FastBPaxos}, server 2 executes $d_2$, $a_2$, $p_2$, and $r_2$. As
illustrated in \figref{FastBPaxos}, the protocol executes as follows:

\begin{itemize}
  \item \textbf{(1)}
    When a client wants to propose a state machine command $x$, it sends $x$ to
    \emph{any} of the proposers.

  \item \textbf{(2)}
    When a proposer $p_i$ receives a command $x$, from a client, it places $x$
    in a vertex with globally unique vertex id $v_x = (p_i, m)$ where $m$ is a
    monotonically increasing integer local to $p_i$. $p_i$ then sends $v_x$ and
    $x$ to all of the dependency service nodes. Note that we predetermine
    that proposer $p_i$ is the leader of round 0 and 1 of the Fast Paxos
    instance associated with vertex $v_x = (p_i, m)$.

  \item \textbf{(3)}
    When a dependency service node $d_j$ receives a command $x$ in vertex
    $v_x$, it computes a set of dependencies $\deps{v_x}$ in the exact same way
    as in Simple \BPaxos{} (i.e.\ $d_j$ maintains an acyclic conflict graph).
    Unlike Simple \BPaxos{} however, $d_j$ does not send $\deps{v_x}$ back to
    the proposer. Instead, it proposes to the co-located Fast Paxos acceptor
    $a_j$ that the value $(x, \deps{v_x})$ be chosen in the instance of Fast
    Paxos associated with vertex $v_x$ in round 0.

  \item \textbf{(4)}
    Fast \BPaxos{} acceptors are unmodified Fast Paxos acceptors. In the
    normal case, when an acceptor $a_j$ receives value $(x, \deps{v_x})$ in
    vertex $v_x = (p_i, m)$, it votes for the value and sends the vote to
    $p_i$.

  \item \textbf{(5)}
    Fast \BPaxos{} proposers are unmodified Fast Paxos proposers. In the
    normal case, $p_i$ receives $f + \maj{f+1}$ votes for value $(x,
    \deps{v_x})$ in vertex $v_x$, so $(x, \deps{v_x})$ is chosen. The proposer
    broadcasts the command and dependencies to the replicas.
    %
    If $p_i$ receives $f + \maj{f+1}$ votes, but they are not all for the same
    value, the proposer executes coordinate recovery (see
    \algoref{FastPaxosProposer} \lineref{CoordinatedRecovery1} --
    \lineref{CoordinatedRecovery2}).

  \item \textbf{(6)}
    Fast \BPaxos{} replicas are identical to Simple \BPaxos{} replicas.
    Replicas maintain and execute conflict graphs, returning the results of
    executing commands to the clients.
\end{itemize}

Note that \figref{FastBPaxos} illustrates six steps of execution, but the
commit time is only four network delays (drawn in red). Communication between
co-located components (e.g., between $d_1$ and $a_1$ and between $p_1$ and
$r_1$) does not involve the network and therefore does not contribute to the
commit time.

\subsection{Recovery}
As with Simple \BPaxos{}, it is possible that a command $y$ chosen in vertex
$v_y$ depends on an unchosen vertex $v_x$ that must be recovered for execution
to proceed. Fast \BPaxos{} performs recovery in the same way as Simple
\BPaxos{}. If a replica detects that a vertex $v_x$ has been unchosen for some
time, it notifies a proposer. The proposer then executes a Fast Paxos recovery
to get a $\noop{}$ chosen in vertex $v_x$ with no dependencies.

\subsection{Lack of Safety}
{\input{figures/fast_bpaxos_bug.tex}}

We now demonstrate why Fast \BPaxos{} is unsafe. Consider the execution of Fast
\BPaxos{} ($f = 2$) illustrated in \figref{FastBPaxosBug}.
\begin{itemize}
  \item
    In \figref{FastBPaxosBug1}, proposer $p_1$ receives command $x$ from a
    client. It places $x$ in vertex $v_x$ and sends $v_x$ and $x$ to the
    dependency service. $d_1$ and $d_2$ receive the message. They place $x$ in
    their conflict graphs without any dependencies, and send the value $(x,
    \emptyset{})$ to their co-located acceptors. $a_1$ and $a_2$ vote for $(x,
    \emptyset)$ in vertex $v_x$, but $p_1$ crashes before it receives any of
    these votes. The messages sent to $d_3$, $d_4$, and $d_5$ are dropped by
    the network.

 \item
    Similarly in \figref{FastBPaxosBug1}, proposer $p_5$ receives a conflicting
    command $y$, $p_5$ sends $v_y$ and $y$ to $d_4$ and $d_5$, $d_4$ and $d_5$
    propose $(y, \emptyset{})$  to $a_4$ and $a_5$, $a_4$ and $a_5$ vote for
    the proposed values, and $p_5$ fails.

  \item
    In \figref{FastBPaxosBug2}, $p_2$ recovers vertex $v_x$. To recover $v_x$,
    $p_2$ executes Phase 1 of Fast Paxos with acceptors $a_1$, $a_2$, and
    $a_3$. $p_2$ observes that $a_1$ and $a_2$ both voted for the value $(x,
    \emptyset{})$ in round $0$. Therefore, $p_2$ concludes that $(x,
    \emptyset{})$ may have been chosen in round $0$, so it proceeds to Phase 2
    and gets the value $(x, \emptyset)$ chosen in vertex $v_x$
    (\algoref{FastPaxosProposer} \lineref{majority}). $p_2$ cannot propose any
    other value (e.g., a $\noop$) because $(x, \emptyset{})$ may have already
    % Addresses Reviewer 1.
    %
    % > In the bullet starting with ``In Figure 14b,'' $\Rightarrow$ remind
    % > readers about Paxos protocol. I think that's necessary to understand why
    % > ``so is forced to propose $(x, p_i)$''.
    been chosen. \markrevisions{This is a core invariant of Paxos.} From our
    omniscient view of the execution, we know that $(x, \emptyset)$ was never
    chosen, but from $p_2$'s myopic view, it cannot make this determination and
    so is forced to propose $(x, \emptyset{})$. This is a \textbf{critical
    point} in the execution, which we will discuss further in a moment.

  \item
    In \figref{FastBPaxosBug2}, proposer $p_4$ recovers vertex $v_y$ in much
    the same way as $p_2$ recovers $v_x$. $p_4$ observes that $a_4$ and $a_5$
    voted for $(y, \emptyset)$ in round 0, so it is forced to get the value
    $(y, \emptyset{})$ chosen.

  \item
    Finally in \figref{FastBPaxosBug2}, proposers $p_2$ and $p_4$ broadcast $(x,
    \emptyset)$ and $(y, \emptyset)$ to all of the replicas. The replicas place
    $x$ and $y$ in their conflict graphs without edges between them. This
    violates the dependency invariant. $x$ and $y$ conflict, so there must be
    an edge between them. Without an edge, the replicas can execute $x$ and $y$
    in different orders, causing their states to diverge.
\end{itemize}

What went wrong? When $p_2$ was recovering $v_x$, Fast Paxos forced it to
choose $(x, \emptyset{})$. However, the dependencies $\deps{v_x} = \emptyset{}$
were \emph{not} computed by a majority of the dependency service nodes. They
were computed only by $d_1$ and $d_2$. This is what allowed conflicting
commands $x$ and $y$ to be chosen without a dependency on each other.

This example illustrates a \defword{fundamental tension} between preserving
the consensus invariant (\invref{ConsensusInvariant}) and preserving the
dependency invariant (\invref{DependencyInvariant}). Maintaining the consensus
invariant in isolation is easy (e.g., use Paxos), and maintaining the
dependency invariant in isolation is also easy (e.g., use the dependency
service). But, maintaining both invariants simultaneously is tricky.

When performing a recovery, like the one in our example above, a proposer is
sometimes forced to propose a particular value (e.g., $(x, \emptyset{})$) in
order to properly implement consensus and simultaneously forced \emph{not} to
propose the value in order to correctly compute dependencies.
%
Resolving the tension between the consensus and dependency invariants during
recovery is the single most important and the single most challenging aspect of
generalized multi-leader protocols like EPaxos, Caesar, and Atlas. All of these
protocols have a similar structure and execution on the normal path. They all
compute dependencies from a majority of servers, and they all execute Fast
Paxos variants to get these dependencies chosen. If you understand the normal
case execution of one of these protocols, it is not difficult to understand the
others. The key feature that distinguishes the protocols is how they resolve
the fundamental tension between implementing consensus and computing
dependencies.

These protocols all take different approaches to resolving the tension. In the
next two sections, we broadly categorize the approaches into two main
techniques: \emph{tension avoidance} and \emph{tension resolution}.  Tension
avoidance involves cleverly manipulating quorum sizes to avoid the tension
entirely. This approach is used by Basic EPaxos~\cite{moraru2013proof} and
Atlas~\cite{enes2020state}. The second technique, tension resolution, is
significantly more complicated and involves detecting and resolving the tension
through various means.
