\section{Related Work}
\newcommand{\condensedparagraph}[1]{\textbf{#1}\;}

\paragraph{MultiPaxos, Raft, Viewstamped Replication}

Generalized multi-leader protocols have a number of advantages over single
leader protocols like MultiPaxos~\cite{lamport2001paxos},
Raft~\cite{ongaro2014search}, and Viewstamped
Replication~\cite{liskov2012viewstamped} that totally order commands into a
log. See~\cite{moraru2013there} for more details and experimental validation.

First, generalized multi-leader protocols avoid being bottlenecked by a single
leader. In protocols like MultiPaxos and Raft, all state machine commands are
funneled through a single leader, making the leader the throughput bottleneck.
In multi-leader protocols on the other hand, state machine commands can be
processed by \emph{any} of the multiple leaders, preventing any one leader from
becoming a bottleneck. This allows multi-leader protocols to achieve higher
throughput.

Second, generalized multi-leader protocols like EPaxos are more resilient to
leader failures. With protocols like MultiPaxos and Raft, when the leader
fails, the protocol's throughput drops to zero and stays at zero until the
failure is detected and a new leader is elected. Depending on the deployment,
this delay could be seconds or minutes. With protocols like EPaxos on the other
hand, when a leader fails, the protocol's throughput drops, but not to zero.
All other non-failed leaders can still process commands, so the throughput
remains high. When the failed leader is replaced, the throughput returns to
normal.

Third, generalized multi-leader protocols achieve lower latency in
geo-distributed applications. Consider a geo-replicated deployment of
MultiPaxos. If the MultiPaxos leader is in Europe, the clients in North America
will experience much higher latency than the clients in Europe. In general, the
clients that are geographically close to the single leader will experience low
latency, while all other clients will experience significantly higher latency.
With generalized multi-leader protocols, the multiple leaders can be
distributed across the deployment so that every client has a leader that is
geographically close by. This reduces the overall latency of the protocol.

Fourth, generalized multi-leader protocols have lower tail latencies for
applications with little interdependence between commands. With protocols like
MultiPaxos, if a single log entry is delayed (e.g., because of a network
failure), all subsequent commands in the log are also delayed. Thus, any
slowdown in the execution of a single command can affect many commands
serialized after it. With generalized multi-leader protocols, independent
commands are executed independently and do not wait for each other. Thus, if a
single command is slow to execute, the other independent commands are not
affected.

\paragraph{A Family of Leaderless Generalized Consensus Algorithms}
In~\cite{losa2016brief}, Losa et al.\ propose a generic generalized consensus
algorithm that is structured as the composition of a generic dependency-set
algorithm and a generic map-agreement algorithm. The invariants of the
dependency-set and map-agreement algorithm are very similar to the consensus
and dependency invariants, and the example implementations of these two
algorithms in~\cite{losa2016brief} form an algorithm similar to Simple
\BPaxos{}. Our paper builds on this body of work by introducing Fast \BPaxos{},
Unanimous \BPaxos{}, and Majority Commit \BPaxos{}. We also identify the
tension between the two invariants as the key distinguishing feature of most
protocols and taxonimize existing protocols by how they handle the tension.

\paragraph{Generalized Paxos and GPaxos}
Generalized Paxos~\cite{lamport2005generalized} and GPaxos~\cite{sutra2011fast}
are generalized protocols but are not fully multi-leader. Clients send commands
directly to acceptors, behaving very much like a leader. However, in the face
of collisions, Generalized Paxos and GPaxos rely on a single leader to resolve
the collision. This single leader becomes a bottleneck in high contention
workloads and prevents scaling.

\paragraph{SpecPaxos, NOPaxos, CURP}
SpecPaxos~\cite{ports2015designing} and NOPaxos~\cite{li2016just} combine
speculative execution and ideas from Fast Paxos in order to reduce commit delay
as low as two network delays. CURP~\cite{park2019exploiting} further introduces
generalization, allowing commuting commands to be executed in any order. These
protocols represent yet another point in the design space of state machine
replication protocols. As the commit delay decreases, the complexity of the
protocols generally increases. We think this is an exciting avenue of research
and hope that an improved understanding of generalized multi-leader protocols
can accelerate research in this direction.

\paragraph{Mencius}
Mencius~\cite{mao2008mencius} is a multi-leader, non-generalized protocol in
which MultiPaxos log entries are round-robin partitioned among a set of
leaders. Because Mencius is not generalized, a log entry cannot be executed
until \emph{all} previous log entries have been executed. To ensure log entries
are being filled in appropriately, Mencius leaders perform all-to-all
communication between each other. Mencius is significantly less complex that
generalized multi-leader protocols like EPaxos, Caesar, and Atlas. This
demonstrates that much of the complexity of these protocols come from being
generalized rather than being multi-leader, though both play a role.

\paragraph{Chain Replication}
Chain Replication~\cite{van2004chain} is a state machine replication protocol
in which the set of state machine replicas are arranged in a totally ordered
chain. Writes are propagated through the chain from head to tail, and reads are
serviced exclusively by the tail. Chain Replication has high throughput
compared to MultiPaxos because load is more evenly distributed between the
replicas. This shows that the leader bottleneck can be addressed without
necessarily having multiple leaders.

\paragraph{Scalog}
Scalog~\cite{ding2020scalog} is a replicated shared log protocol that achieves
high throughput using a sophisticated form of batching. A client does not send
values directly to a centralized leader for sequencing in the log. Instead, the
client sends its values to one of a number of batchers. Periodically, the
batchers' batches are sealed and assigned an id. This id is then sent to a
state machine replication protocol, like MultiPaxos, for sequencing. Like
Mencius, Scalog represents a way to avoid a leader bottleneck without needing
multiple leaders.

\paragraph{PQR, Harmonia, and CRAQ}
PQR~\cite{charapko2019linearizable}, Harmonia~\cite{zhu2019harmonia}, and
CRAQ~\cite{terrace2009object} all implement optimizations so that reads (i.e.\
state machine commands that do not modify the state of the state machine) can
be executed without contacting a leader, while writes are still processed by
the leader. An interesting direction of future work could explore whether or
not these read optimizations could be applied to generalized multi-leader
protocols.
