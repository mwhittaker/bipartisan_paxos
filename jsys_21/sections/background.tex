\section{Background}\seclabel{Background}

\subsection{System Model}
Throughout the paper, we assume an asynchronous network model in which messages
can be arbitrarily dropped, delayed, and reordered. We assume machines can fail
by crashing but do not act maliciously; i.e., we do not consider Byzantine
failures. We assume that machines operate at arbitrary speeds, and we do not
assume clock synchronization. Every protocol discussed in this paper assumes
that at most $f$ machines will fail for some configurable $f$.

\subsection{Paxos}
A \defword{consensus protocol} is a protocol that selects a single value from a
set of proposed values. \defword{Paxos}~\cite{lamport1998part,
lamport2001paxos} is one of the oldest and most popular consensus protocols. A
Paxos deployment that tolerates $f$ faults consists of an arbitrary number of
clients, $f+1$ nodes called \defword{proposers}, and $2f+1$ nodes called
\defword{acceptors}, as illustrated in \figref{PaxosBackgroundDiagram}.
%
To reach consensus on a value, an execution of Paxos is divided into a number
of integer valued rounds, each round having two phases: Phase 1 and Phase 2.
Every round is orchestrated by a single pre-determined proposer. For example,
with two proposers $p_1$ and $p_2$, we can predetermine that $p_1$ is
responsible for executing even rounds (i.e.\ $0$, $2$, $4$, $\ldots$), while
$p_2$ is responsible for executing odd rounds (i.e.\ $1$, $3$, $5$, $\ldots$).

{\input{figures/paxos_background_diagram.tex}}

When a proposer executes a round, say round $i$, it attempts to get some value
$x$ chosen in that round. Paxos is a consensus protocol, so it must only choose
a single value. Thus, Paxos must ensure that if a value $x$ is chosen in round
$i$, then no other value besides $x$ can ever be chosen in any round less than
$i$. This is the purpose of Paxos' two phases. In Phase 1 of round $i$, the
proposer contacts the acceptors to (a) learn of any value that may have already
been chosen in any round less than $i$ and (b) prevent any new values from
being chosen in any round less than $i$. In Phase 2, the proposer proposes a
value to the acceptors, and the acceptors vote on whether or not to choose it.
In Phase 2, the proposer is careful to only propose a value $x$ if it learned
through Phase 1 that no other value has been or will be chosen in a previous
round.

More concretely, Paxos executes as follows. When a client wants to propose a
value $x$, it sends $x$ to a proposer $p$. Upon receiving $x$, $p$ begins
executing one round of Paxos, say round $i$. First, it executes Phase 1. It
sends $\PhaseIA{i}$ messages to at least a majority of the acceptors. An
acceptor ignores a $\PhaseIA{i}$ message if it has already received a message
in a larger round. Otherwise, it replies with a $\PhaseIB{i}{vr}{vv}$ message
containing the largest round $vr$ in which the acceptor voted and the value it
voted for, $vv$. If the acceptor hasn't voted yet, then $vr = -1$ and $vv = $
\textsf{null}. When the proposer receives \textsc{Phase1b} messages from a
majority of the acceptors, Phase 1 ends and Phase 2 begins.

% Phase 1 serves two purposes.
% \begin{enumerate}[1.]
%   \item
%     The proposer $p$ learns of any value $v$ that may have already been chosen
%     in in some round less than $i$. If any such value exists, then the proposer
%     must propose it in Phase 2 (if it proposed some other value, then two
%     different values could end up chosen).
%
%   \item
%     Phase 1 prevents any new values from being chosen in any round less than
%     $i$.
% \end{enumerate}
% These two properties are \textbf{critical} to ensure that Paxos chooses at most
% one value.

At the start of Phase 2, the proposer uses the \textsc{Phase1B} messages that
it received in Phase 1 to select a value $x$ such that no value other than $x$
has been or will be chosen in any round less than $i$. Specifically $x$ is the
vote value associated with the largest received vote round, or any value if no
acceptor had voted (see~\cite{lamport2001paxos} for details). Then, the
proposer sends $\PhaseIIA{i}{x}$ messages to at least a majority of the
acceptors. An acceptor ignores a $\PhaseIIA{i}{x}$ message if it has already
received a message in a larger round. Otherwise, it votes for $x$ and sends
back a $\PhaseIIB{i}$ message to the proposer. If a majority of acceptors vote
for the value (i.e.\ if the proposer receives $\PhaseIIB{i}$ messages from at
least a majority of the acceptors), then the value is chosen, and the proposer
informs the client. This execution is illustrated in
\figref{PaxosBackgroundDiagram}.

\TODO[mwhittaker]{Find a citation for this optimization. Heidi's thesis?}
Note that it is safe for the leader of round $0$ (the smallest round) to skip
Phase 1 and proceed directly to Phase 2. Recall that the leader of round $i$
executes Phase 1 to learn of any value that may have already been chosen in any
round less than $i$ and to prevent any new values from being chosen in any
round less than $i$. There are no rounds less than $0$, so these properties are
satisfied trivially. This is a common optimization.

\subsection{Flexible Paxos}
Paxos deploys a set of $2f+1$ acceptors, and proposers communicate with at
least a \emph{majority} of the acceptors in Phase 1 and in Phase 2.
\defword{Flexible Paxos}~\cite{howard2017flexible} is a Paxos variant that
generalizes the notion of a \emph{majority} to that of a \emph{quorum}.

Specifically, Flexible Paxos introduces the notion of a \defword{read-write
quorum system} $(P1; P2)$~\cite{vukolic2013origin}. Given a set $A$ of
acceptors, $P1$ and $P2$ are sets of \defword{quorums}, where each quorum is a
subset of $A$. A read-write quorum system satisfies the property that every
quorum in $P1$ (known as a \defword{Phase 1 quorum}) intersects every quorum in
$P2$ (known as a \defword{Phase 2 quorum}).

Flexible Paxos is identical to Paxos with the exception that proposers now
communicate with an arbitrary Phase 1 quorum in Phase 1 and an arbitrary Phase
2 quorum in Phase 2. In the remainder of this paper, we assume that all
protocols operate using quorums from an arbitrary read-write quorum system
rather than majorities from a fixed set of $2f+1$ acceptors. So when we say
Paxos, for example, we really mean Flexible Paxos.

%
% For example,
% \[
%   C = (
%     \underbrace{\set{a_1, a_2}}_A;
%     \underbrace{\set{\set{a_1, a_2}}}_{P1};
%     \underbrace{\set{\set{a_1}, \set{a_2}}}_{P2}
%   )
% \]
% is a configuration with two acceptors $a_1$ and $a_2$. There is a single Phase
% 1 quorum that includes both acceptors, and there are two singleton Phase 2
% quorums. On the other hand
% \[
%   C = (\set{a_1, a_2}; \set{\set{a_1}}; \set{\set{a_1, a_2}, \set{a_2}})
% \]
% is \emph{not} a configuration since the Phase 1 quorum $\set{a_1}$ does not
% intersect the Phase 2 quorum $\set{a_2}$.

% \NOTE[heidi]{I might be simpler to define a configuration as just the set of
% phase-2 quorums. So for example, $C=\{\{a_1\},\{a_2\}\}$. Then you can union
% configurations together to calculate the set of quorum to intersect with in
% phase-1.}

% \subsection{Reconfiguration}
% \defword{Reconfiguring} a protocol is the act of changing the set of nodes that
% are participating in the protocol, while the protocol executes. For example,
% we may want to reconfigure a MultiPaxos deployment from acceptors $a_1$, $a_2$,
% $a_3$ to acceptors $a_1$, $a_2$, $a_4$ if acceptor $a_3$ has crashed. There are
% a number of reasons to perform a reconfiguration.  Reconfiguration can be used
% to replace failed nodes. Reconfiguration can be used to increase the fault
% tolerance of a protocol like MultiPaxos by increasing $f$ (and hence the number
% of nodes). Reconfiguration can be used to keep nodes geographically close to
% clients, thereby reducing latency. If a protocol is managed by a job scheduler
% like Borg~\cite{verma2015large} or is deployed in a serverless environment,
% reconfiguration allows the protocol to tolerate frequent preemptions.

\subsection{MultiPaxos}
Whereas Paxos is a consensus protocol that agrees on a single value,
\defword{MultiPaxos}~\cite{lamport1998part, van2015paxos} is a \defword{state
machine replication protocol} that agrees on a sequence, or ``log'' of values.
MultiPaxos manages multiple \defword{replicas} of a deterministic state
machine. Clients send state machine commands to MultiPaxos, MultiPaxos places
the commands in a totally ordered log, and state machine replicas execute the
commands in log order. By beginning in the same initial state and executing the
same commands in the same order, all state machine replicas are kept in sync.
%
% \NOTE[heidi]{defword replica given than acceptor and proposer are defworded.}
%
% \NOTE[heidi]{i'd suggest skipping explicit replicas and just assuming they
% are co-located with proposers (as you later assumed) as replicas are never
% mentioned again.}
%
% I want to emphasize that proposers and replicas are split since we only need
% f+1 proposers, but for garbage collection later, we need 2f+1 replicas.

{\input{figures/multipaxos_background_diagram.tex}}

To agree on a log of commands, MultiPaxos implements one instance of Paxos for
every log entry. The $i$th instance of Paxos chooses the command in log entry
$i$. More concretely, a MultiPaxos deployment that tolerates $f$ faults
consists of an arbitrary number of clients, at least $f+1$ proposers, $2f+1$
acceptors, and at least $f+1$ replicas, as illustrated in
\figref{MultiPaxosBackgroundDiagram}.

One of the proposers is elected leader in some round, say round $i$. We assume
the leader knows that log entries up to and including log entry $w$ have
already been chosen (e.g., by communicating with previous leaders, or by
communicating with the replicas). The leader then runs Phase 1 of Paxos in
round $i$ for \emph{every} log entry larger than $w$. Note that even though
there are an infinite number of log entries larger than $w$, the leader can
execute Phase 1 using a finite amount of information.  In particular, the
leader sends a single $\PhaseIA{i}$ message that acts as the \textsc{Phase1A}
message for every log entry larger than $w$. Also, an acceptor replies with a
$\PhaseIB{i}{vr}{vv}$ message only for log entries in which the acceptor has
voted. The infinitely many log entries in which the acceptor has not yet voted
do not yield an explicit \textsc{Phase1B} message.

% \NOTE[heidi]{this description confused me a bit as it seems to assume everyone
% is agreed on the index $w$.}
%
% Removed the mention of $w$ when discussing the acceptors, so hopefully it's
% clearer now.

The leader's knowledge about the log after Phase 1 is shown in
\figref{MultiPaxosPhase1Log}. The leader knows that a prefix of the log (up to
and including log entry $w$) has already been chosen. Then, there is a
subsequence of the log that contains commands that may have already been chosen
in a round less than $i$. This subsequence may contain unchosen entries, which
we call ``holes''. Finally, the log has an infinite tail of empty entries in
which the leader knows no command has previously been chosen.

{\input{figures/multipaxos_phase1_log.tex}}

After Phase 1, the leader sends a \textsc{Phase2A} message for every log entry
in the middle subsequence of potentially chosen commands, proposing a ``no-op''
command for the holes. Simultaneously, the leader begins accepting client
requests. When a client wants to propose a state machine command, it sends the
command to the leader. The leader assigns log entries to commands in increasing
order, beginning at the start of the infinite tail of unchosen entries. It then
runs Phase 2 of Paxos to get the command chosen in that entry in round $i$.
Once the leader learns that a command has been chosen in a given log entry, it
informs the replicas. Replicas insert chosen commands into their logs and
execute the logs in prefix order, sending the results of execution back to the
clients. This execution is illustrated in \figref{MultiPaxosBackgroundDiagram}.

% \NOTE[heidi]{you may wish have acceptors only accept proposals in-order to
% simplify the description by avoiding log gaps.}
%
% Sadly, the i/i+1 optimization later messes this up since we let acceptors
% start processing commands at a random point in the log.

It is critical to note that a leader performs Phase 1 of Paxos only once
\emph{per round}, not once \emph{per command}. In other words, Phase 1 is not
performed during normal operation. It is performed only when
the leader fails and new leader is elected in a larger round, an uncommon
occurrence.
