\section{A Primer on State Machine Replication}\seclabel{Background}

Throughout the paper, we assume a system model in which messages can be
arbitrarily dropped, delayed, and reordered. We assume machines can fail by
crashing but do not act maliciously; i.e., we do not consider Byzantine
failures. We assume that machines operate at arbitrary speeds, and we do not
assume clock synchronization. Every protocol discussed in this paper assumes
that at most $f$ machines will fail for some configurable $f$.
% Addresses Reviewer 2.
%>
% > ``This paper assumes that at most $f$ machines will fail'' - from a practical
% > standpoint it's impossible to guarantee; it's better to mention what happens
% > then this threshold is passed if consistency or availability becomes
% > violated.
\markrevisions{If more than $f$ machines fail, the protocols remain safe,
but won't be live.}

\subsection{State Machine Replication}
\defword{State machine replication} is the act of choosing a sequence (a.k.a.\
log) of values. A state machine replication protocol manages a number of
copies, or \defword{replicas}, of a deterministic state machine. Over time, the
protocol constructs a growing log of state machine commands, and replicas
execute the commands in log order. By beginning in the same initial state, and
by executing the exact same commands in the exact same order, all of the state
machine replicas are kept in sync. This is illustrated in \figref{LogOverTime}.

{\input{figures/log_over_time.tex}}

State machine replication builds on the simpler problem of \defword{consensus}.
Rather than choosing a sequence of values, consensus involves choosing a single
value. State machine replication protocols like MultiPaxos implement state
machine replication using one instance of consensus for every log entry, so to
understand state machine replication, we must first understand consensus. We
review Paxos, the most popular consensus algorithm, and then extend Paxos to
MultiPaxos.

\subsection{Paxos}
A Paxos~\cite{lamport1998part} deployment that tolerates $f$ faults consists of
an arbitrary number of clients, $f+1$ nodes called \defword{proposers}, and
$2f+1$ nodes called \defword{acceptors}, as illustrated in \figref{Paxos}.
%
To reach consensus on a value, an execution of Paxos is divided into a number
% Addresses Reviewer 2.
%
% > ``To reach is consensus on a value, an execution of Paxos is divided into a
% > number of integer value rounds'' - it's a clever idea to pre-assign a set of
% > ballot numbers to the machines but using a term ``round'' both for a ballot
% > number and an act of communication is confusing. It may create a false
% > impression that it's impossible to skip ballot numbers. Maybe it'd be better
% > to mention that rounds also known as ballot numbers (paxos) or term (raft)
% > and add a reference to ``Paxos vs Raft: Have we reached consensus on
% > distributed consensus?'' to use it as a map between different protocols.
of integer valued rounds \markrevisions{(also known as ballots, epochs, terms,
views, etc.~\cite{howard2020paxos})}. Every round has two phases, Phase 1 and
Phase 2, and every round is orchestrated by a single pre-determined proposer.
% Addresses Reviewer 1.
%
% > ``For example, with two proposers... the leader of the round'' seems not
% > important.
%
% For example, with two proposers $p_1$ and $p_2$, we can predetermine that
% $p_1$ is responsible for executing even rounds (i.e.\ $0$, $2$, $4$,
% $\ldots$), while $p_2$ is responsible for executing odd rounds (i.e.\ $1$,
% $3$, $5$, $\ldots$).
If a proposer is responsible for executing a round, we sometimes say the
proposer is the leader of the round.

{\input{figures/paxos.tex}}

When a proposer executes a round, say round $i$, it attempts to get some value
% Addresses Reviewer 1.
%
% > ``chosen'' at the end is not defined. It seems different from ``chosen''
% > in page 4. I would clarify ``chosen'' and ``voted'' in page 3.
$v$ \markrevisions{``chosen''} in that round. \markrevisions{We'll define
formally what it means for a value to be chosen momentarily.} Paxos is a
consensus protocol, so it must only choose a single value. Thus, Paxos must
ensure that if a value $v$ is chosen in round $i$, then no other value besides
$v$ can ever be chosen in any round less than $i$. This is the purpose of
Paxos' two phases. In Phase 1 of round $i$, the proposer contacts the acceptors
to (a) learn of any value that may have already been chosen in any round less
than $i$ and (b) prevent any new values from being chosen in any round less
than $i$. In Phase 2, the proposer proposes a value to the acceptors, and the
acceptors vote on whether or not to choose it.  In Phase 2, the proposer is
careful to only propose a value $v$ if it learned through Phase 1 that no other
value has been or will be chosen in a previous round.

More concretely, Paxos executes as follows. When a client wants to propose a
value $v$, it sends $v$ to a proposer $p$. Upon receiving $v$, $p$ begins
executing one round of Paxos, say round $i$. First, it executes Phase 1. It
% Addresses Reviewer 3.
%
% > I am unsure why at line 21 of Algorithm 1, the propose is sending to at
% > least $f+1$ acceptors instead of all acceptors. As $f$ of the acceptors
% > may crash, so sending to all acceptors in important.
sends $\PhaseIA{i}$ messages to \markrevisions{the acceptors}. An acceptor
ignores a $\PhaseIA{i}$ message if it has already received a message in a
larger round.  Otherwise, it replies with a $\PhaseIB{i}{vr}{vv}$ message
containing the largest round $vr$ in which the acceptor voted and the value it
voted for, $vv$. If the acceptor hasn't voted yet, then $vr = -1$ and $vv = $
\textsf{null}. When the proposer receives \textsc{Phase1B} messages from a
majority of the acceptors, Phase 1 ends and Phase 2 begins.

At the start of Phase 2, the proposer uses the \textsc{Phase1B} messages that
it received in Phase 1 to select a value $v$ such that no value other than $v$
has been or will be chosen in any round less than $i$. Specifically $v$ is the
vote value associated with the largest received vote round, or any value if no
acceptor had voted (see~\cite{lamport2001paxos} for details). Then, the
% Addresses Reviewer 3.
%
% > I am unsure why at line 21 of Algorithm 1, the propose is sending to at
% > least $f+1$ acceptors instead of all acceptors. As $f$ of the acceptors
% > may crash, so sending to all acceptors in important.
proposer sends $\PhaseIIA{i}{v}$ messages to \markrevisions{the acceptors}. An
acceptor ignores a $\PhaseIIA{i}{v}$ message if it has already received a
message in a larger round. Otherwise, it votes for $v$ and sends back a
$\PhaseIIB{i}$ message to the proposer. If a majority of acceptors vote for the
value (i.e.\ if the proposer receives $\PhaseIIB{i}$ messages from a
% Addresses Reviewer 1.
%
% > ``chosen'' at the end is not defined. It seems different from ``chosen''
% > in page 4. I would clarify ``chosen'' and ``voted'' in page 3.
majority of the acceptors), then the value is chosen\markrevisions{---this is
the formal definition of when a value is chosen---}and the proposer informs the
client. This execution is illustrated in \figref{Paxos}.
% Addresses Reviewer 1:
%
% > If any message gets ignored, what does a proposer do?
\markrevisions{%
If the proposer does not receive sufficiently many \textsc{Phase1B} or
\textsc{Phase2B} responses from the acceptors (e.g., because of network
partitions or dueling proposers), then the proposer restarts the protocol in a
larger round.
}

Note that it is safe for the leader of round $0$ (the smallest round) to skip
Phase 1 and proceed directly to Phase 2. Recall that the leader of round $i$
executes Phase 1 to learn of any value that may have already been chosen in any
round less than $i$ and to prevent any new values from being chosen in any
round less than $i$. There are no rounds less than $0$, so these properties are
satisfied vacuously.

\subsection{MultiPaxos}
As mentioned earlier, MultiPaxos uses one instance of Paxos for every log
entry, choosing the command in the $i$th log entry using the $i$th instance of
Paxos.  A MultiPaxos deployment that tolerates $f$ faults consists of an
arbitrary number of clients, at least $f+1$ proposers, and $2f+1$ acceptors
(like Paxos), as well as at least $f+1$ replicas, as illustrated in
\figref{MultiPaxos}.

{\input{figures/multipaxos.tex}}

Initially, one of the proposers is elected leader and runs Phase 1 of Paxos for
every log entry. Though there are an infinite number of log entries, all but a
finite prefix of the log entries are empty, so the leader can run Phase 1 for
all log entries with only a small number of messages.
%
When a client wants to propose a state machine command
$x$, it sends the command to the leader (1). The leader assigns the command a
log entry $i$ and then runs Phase 2 of the $i$th Paxos instance to get the
value $x$ chosen in entry $i$. That is, the leader sends \msgfont{Phase2A}
messages to the acceptors to vote for value $x$ in slot $i$ (2). In the normal
case, the acceptors all vote for $x$ in slot $i$ and respond with
\msgfont{Phase2B} messages (3). Once the leader learns that a command has
been chosen in a given log entry, it informs the replicas (4). Replicas insert
commands into their logs and execute the logs in prefix order.

Note that every command is sent to the leader, and the leader performs
disproportionally more work per command compared to the other nodes in the
protocol. For example, in \figref{MultiPaxos}, the leader must send and receive
a total of 7 messages per command while the acceptors and replicas send and
receive at most 2. This is why the MultiPaxos leader is a well known throughput
bottleneck~\cite{mao2008mencius, moraru2013there}.
