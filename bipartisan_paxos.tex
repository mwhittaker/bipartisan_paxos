\documentclass{mwhittaker}
\title{Bipartisan Paxos}

\usepackage{pervasives}
\usepackage{tikz}
\usetikzlibrary{calc}
\usetikzlibrary{positioning}
\usetikzlibrary{shapes.misc}

\begin{document}
\maketitle

\section{Overview}
Egalitarian Paxos~\cite{moraru2013there}, or EPaxos, is a state machine
replication protocol like Raft~\cite{ongaro2014search} or Viewstamped
Replication~\cite{liskov2012viewstamped}. An EPaxos instance consists of a
fixed set of $n = 2f + 1$ nodes, where $f$ denotes the maximum allowable number
of node failures. With EPaxos, clients forward state machine commands to one of
the $n$ nodes. When a node receives a command $a$ from a client, it can get the
command $a$ chosen after one round trip of communication to a superquorum of
the $n$ nodes if no other concurrently proposed command conflicts with $a$. If
there is a concurrently proposed command that conflicts with $a$, then the node
can get $a$ chosen after potentially one additional round trip of communication
to a quorum (just a quorum, not a superquorum) of the $n$ nodes.

EPaxos has a number of nice features that set it apart from other state machine
replication protocols. For example, it can choose non-conflicting commands in
one round trip, disjoint sets of conflicting commands do not affect each other,
and superquorum sizes are one node smaller than the superquorum sizes used by
other protocols.
%
In this paper, we describe a family of consensus algorithms that are derived
from EPaxos. We call them Bipartisan Paxos, Unanimous Bipartisan Paxos, and
Fast Bipartisan Paxos.

\section{Bipartisan Paxos}
In this section, we describe Bipartisan Paxos, or BPaxos. BPaxos is designed to
be as simple to understand as possible, even at the cost of performance. Other
variants of BPaxos that we'll see later (i.e.\ Unanimous BPaxos and Fast
BPaxos) improve on BPaxos' performance.

Recall that the EPaxos protocol is divided into two main components. The first
component constructs a directed graph of state machine commands and decides
when certain commands are considered \defword{committed}. The second component
executes the commands in reverse topological order one strongly connected
component at a time. BPaxos steals the second component, the command execution
component, from EPaxos without any modifications. The two algorithms execute
commands in 100\% the same way. Where BPaxos differs from EPaxos is in how it
constructs the graph and decides when commands in the graph are committed.

BPaxos is illustrated in \figref{BPaxos}. A BPaxos instance has three main
components: an ordering service, a consensus service, and a set of BPaxos
nodes. We'll explain each of these three components momentarily, but first we
pause to review the notion of instances, borrowed from EPaxos.

{\input{figures/bpaxos.tex}}

\paragraph{Instances}
As with EPaxos, every BPaxos node $R$ manages a set of numbered
\defword{instances} $R.1$, $R.2$, $R.3$, $\ldots$. As described in
\cite{moraru2013there}, we can visualize the set of instances as a
two-dimensional grid with the set of BPaxos nodes on the $x$-axis and the set
of instance numbers on the $y$-axis, as illustrated in
\figref{BPaxosInstances}.

{\input{figures/instances.tex}}

\paragraph{Ordering Service}
\newcommand{\deps}[1]{\text{deps}(#1)}

The ordering service is responsible for computing the dependencies between
conflicting state machine commands. When a BPaxos node $R$ receives a state
machine command $a$ from a client, it chooses some instance $R.i$ for $a$.
Then, $R$ sends $a$ and $R.i$ to the ordering service. When the ordering
service receives a command $a$ for instance $R.i$, it replies with a tuple $(a,
R.i, \deps{a})$ where $\deps{a} = \set{I_1, \ldots, I_n}$ is a set of instances
that we call $a$'s \defword{dependencies}.

The ordering service provides the following guarantee. If two conflicting
commands $a$ and $b$ in instances $I_a$ and $I_b$ yield responses $(a,
\deps{a})$ and $(b, \deps{b})$ from the ordering service, then either $I_a \in
\deps{b}$ or $I_b \in \deps{a}$ (or both). That is, if two conflicting commands
are sent to the ordering service, at least one is guaranteed to be a dependency
of the other.

Note that the ordering service makes the following assumption. At most one
command can be sent in any given instance. BPaxos satisfies this invariant
because $R$ assigns every command $a$ to a unique instance $R.i$, and no other
EPaxos node contacts the ordering service for an instance $R.i$.

Implementing a fault tolerant ordering service is straightforward. We employ
$2f + 1$ ordering service nodes $o_{1}, \ldots, o_{2f + 1}$. When a BPaxos node
$R$ sends a command $a$ in instance $R.i$ to the ordering service, it sends the
command to all $2f + 1$ of the ordering service nodes. Every ordering service
node $o_i$ maintains a set $O_i = {(c_1, I_1), (c_2, I_2), \ldots}$ of all the
commands and corresponding instances that it has received. When node $o_i$
receives a command $a$ for instance $R.i$ from a BPaxos node, it atomically
performs two actions.
\begin{enumerate}
  \item
    $o_i$ replies to $R$ with the tuple $(a, R.i, \deps{a}_i)$ where
    $\deps{a}_i$ is the set of instances $I$ for which there exists a $(c, I)
    \in O_i$ such that $a$ and $c$ conflict. This is the set of instances that
    $o_i$ has previously received that contain a command that conflict with
    $a$.

  \item
    $o_i$ adds $(a, R.i)$ to $O_i$.
\end{enumerate}

When a BPaxos node receives replies $(a, R.i, \deps{a}_{i_1}), \ldots, (a, R.i,
\deps{a}_{i_{f+1}})$ from a quorum $Q_a$ of $f + 1$ ordering service nodes, it
takes $(a, R.i, \deps{a}_{i_1} \cup \ldots \cup \deps{a}_{i_{f+1}}) = (a,
\deps{a})$ to be the response from the ordering service.

To understand why this ordering service implementation provides its advertised
guarantees, consider conflicting commands $a$ and $b$ in instances $I_a$ and
$I_b$. Assume $a$'s reply $(a, I_a, \deps{a})$ was formed from a quorum $Q_a$
and $b$'s reply $(b, I_b, \deps{b})$ was formed from a quorum $Q_b$. Any two
quorums intersect, so $Q_a \cap Q_b$ is nonempty. Let $o_i$ be an ordering
service node in this intersection. $o_i$ either received $a$ or $b$ first. If
it received $a$ first, then $(a, I_a)$ is in $O_i$ when $o_i$ processed command
$b$, so $o_i$ includes $I_a$ in $\deps{b}_i$, so $I_a$ is in $\deps{b}$.
Symmetrically, if $o_i$ received $b$ first, then it includes $I_b$ in
$\deps{a}$.

\paragraph{Consensus Service}
We assume we have some set $p_1, \ldots, p_n$ of nodes that implement Plain
Jane consensus. A BPaxos node can propose to the consensus service that some
value $v$ be chosen in some instance $I$. The consensus service replies with
the value that has been chosen in instance $I$, which may or may not be $v$
depending on if there are concurrent proposers proposing to instance $i$. The
consensus service guarantees that for every instance $i$, at most one value is
ever chosen in $i$.

\paragraph{BPaxos Nodes}
We assume a fixed set $R_1, \ldots, R_{2f+1}$ of $2f + 1$ BPaxos nodes.
%
Clients sends state machine commands to BPaxos nodes to be executed by the
replicated state machine. When a BPaxos node $R$ receives a command $a$, it
sends the command to the ordering service in a previously unused instance $R.i$
and receives a reply $(a, R.i, \deps{a})$. $R$ then proposes the value $(a,
\deps{a})$ to the consensus service in instance $R.i$. The consensus service
then replies with some chosen value $(a', \deps{a'})$ (which is equal to $(a,
\deps{a})$ in the failure-free case). At this point, the command $a'$ is
considered committed in instance $R.i$ of the directed graph of state machine
commands with outbound edges to instances in $\deps{a'}$. Node $R$ also informs
the other BPaxos nodes that the value $(a', \deps{a'})$ has been committed. As
noted earlier, the execution of the commands in the directed graph is identical
to that of EPaxos. Committed commands are executed in reverse topological
order, one strongly connected component at a time. Within a strongly connected
component, BPaxos executes commands in an arbitrary but deterministic order.
There are no sequence numbers, so BPaxos provides serializability instead of
linearizability. This is not fundamental. It just makes things a bit simpler.

\newcommand{\noop}{\text{noop}}
It's possible that (1) a committed command $a$ depends on an instance $R.i$
that contains an uncommitted command, and (2) the BPaxos node $R$ that manages
the instance $R.i$ has crashed. If the instance $R.i$ remains forever
uncommitted, then the command $a$ will never be executed. To avoid this
liveness violation, if any BPaxos node $S$ notices that instance $R.i$ has been
uncommitted for some time, it can propose to the consensus service that the
value $(\noop, \emptyset)$ be chosen in instance $R.i$ where $\noop$ is a
command that doesn't affect the state machine.

\paragraph{Correctness}
Now we prove that BPaxos is correct by leveraging EPaxos' proof of correctness.
Open up \cite{moraru2013proof} and head to section 5.6, which contains proofs
of EPaxos' correctness.
\begin{itemize}
  \item
    \textbf{Theorem 1} says that EPaxos satisfies nontriviality. Clearly, so
    does BPaxos.

  \item
    \textbf{Lemma 1} says that EPaxos commits a command in an instance only if
    no other command will ever be committed in the same instance. The fact that
    BPaxos satisfies Lemma 1 is immediate from the fact that we use a consensus
    service. In fact, Lemma 1 is really just a restatement of the exact
    property that the consensus service provides.

  \item
    \textbf{Theorem 2} follows from Lemma 1.

  \item
    \textbf{Theorem 3} is trivial.

  \item
    \textbf{Theorem 4} states that if two conflicting commands are both
    committed, then they will be executed in the same order by every BPaxos
    node. The proof that BPaxos satisfies Theorem 4 is more or less the same as
    the proof that EPaxos satisfies theorem 4, which shouldn't be too
    surprising because BPaxos executes commands 100\% identically to EPaxos. In
    short, if two commands conflict, the ordering service guarantees that one
    will depend on the other. If both commands end up in the same strongly
    connected component, they will be executed in the same deterministic order.
    And, if the commands end up in different strongly connected components,
    then one component is guaranteed to be ordered before the other, so the two
    commands are executed in reverse topological order. There's also the
    wrinkle that we may commit a $\noop$, but $\noop$s don't conflict with any
    other commands, so we have nothing to worry about.
\end{itemize}

\section{An Aside on Fast Paxos}
Before we move on, we summarize the relevant bits of Fast
Paxos~\cite{lamport2006fast} that are critical to our remaining BPaxos
variants. In Fast Paxos, after a proposer receives phase 1b messages from a
quorum of acceptors, it performs the following logic to select a value to
propose:
\begin{itemize}
  \item
    Let $k$ be the largest vote round in the quorum of phase 1b messages.
    Let $V$ be the corresponding set of vote values for round $k$.
  \item
    (Case 1) If $V = \set{v}$, then propose $v$.
  \item
    (Case 2) If $V$ contains a value $v$ that may have been chosen in round
    $k$, propose $v$. Note that quorum sizes are selected in such a way that at
    most one such $v$ can exist.
  \item
    (Case 3) Otherwise, propose anything.
\end{itemize}

Proving the correctness of Fast Paxos involves proving the statement $P(i)$
that says that if an acceptor votes for a value $v$ in round $i$, then no
other value besides $v$ has been or will be chosen in any round $j$ less than
$i$. We prove this claim by induction. $P(0)$ is trivial because there are no
rounds less than $0$. For the general case, we perform a case analysis on $j$.
First, assume $k \neq -1$.
\begin{itemize}
  \item
    If $k < j < i$, then no value has been or will be chosen in round $j$
    because a phase 1 quorum of acceptors had not voted in any round larger
    than $k$ and promised not to vote in any round less than $i$.

  \item
    If $k = j$, then we perform a case analysis on the proposer's logic.
    \begin{itemize}
      \item
        (Case 1) If $V = \set{v}$, then a quorum of acceptors have either voted
        for $v$ in round $k$ or promised not to vote in round $k$. Thus, no
        other value besides $v$ can be chosen in round $k$.
      \item
        (Case 2) If $V$ contains a value that may have been chosen in round
        $k$, we propose it. Quorum sizes are set up such that no other value
        could have been chosen in round $k$.
      \item
        (Case 3) No value could have been chosen in round $k$.
    \end{itemize}

  \item
    If $j < k$, we again perform a case analysis on the proposer's logic.
    \begin{itemize}
      \item
        (Case 1) By $P(k)$, no value other than $v$ has been or will be chosen
        in round $j$.
      \item
        (Case 2) Let $v_1, v_2 \in V$. By $P(v_1), P(v_2)$, no value has been
        or will be chosen in round $j$.
      \item
        (Case 3) Same as Case 2.
    \end{itemize}
\end{itemize}
If $k = -1$, then we know that no value has been or will be chosen in any round
less than $i$ by the same line of reasoning as above, so we're free to propose
anything.

Note that we can make the following small tweak to Fast Paxos without
compromising its correctness. We can change Case 1 of the proposer's logic from
``If $V = \set{v}$, then propose $v$'' to ``If $V = \set{v}$ and $k \neq 0$,
then propose $v$''. That is, a proposer will only perform Case 1 if $k \neq 0$.

\section{An Incorrect Optimization}
TODO: Explain the bad tweak and explain the principle of trying to get unstuck.
Then segue into unanimous BPaxos as a simple way to get unstuck. Then describe
fast BPaxos as a fancier way of getting unstuck.

\section{Unanimous Bipartisan Paxos}
\subsection{Tweak 1: Colocation and Paxos}
We'll also warn you right now that we assume you have a good understanding of
Fast Paxos~\cite{lamport2006fast}. If you don't, the tweaks are not going to
make any sense.



For our first tweak, we'll do two things that don't actually improve the
performance of anything, but will set us up nicely for the following tweaks.
First, we choose to implement the consensus service with Paxos where the set
$p_1, \ldots, p_{2f+1}$ of consensus nodes are Paxos acceptors. We'll also
assume that every BPaxos node $R$ initially runs phase 1 of Paxos for every
instance $R.i$, the same trick played by Multi-Paxos. Second, we'll colocate
the ordering service nodes and the Paxos acceptors. We probably want to
colocate the BPaxos nodes with the ordering service nodes and Paxos acceptors
as well, but we won't show this in order to keep things simple.
%
The tweaked BPaxos architecture is illustrated in \figref{ColocatedBPaxos}.
Clearly, this tweak does not affect the correctness of our BPaxos protocol.

{\input{figures/colocated_bpaxos.tex}}

\subsection{Tweak 2: Fast Paxos}
Currently, the commit latency of BPaxos is two round trips in the normal case.
When a BPaxos node receives a command $a$ from a client, it (1) performs one
round trip to a quorum of ordering service nodes to obtain $(a, \deps{a})$, and
then (2) performs another round trip to a quorum of Paxos acceptors to have the
value $(a, \deps{a})$ chosen. Our second tweak will reduce the BPaxos commit
latency from two round trips (to two quorums) to one round trip (to a
superquorum). The idea behind the tweak is to implement our consensus service
using a variant of Fast Paxos with quroums of size $f + 1$ and superquorums of
size $n$.

Upon initializing, every BPaxos node $R$ performs phase $1$ of Fast Paxos for
ballot $0$ for every Fast Paxos instance $R.i$. We let ballot $0$ be a fast
ballot, so after $R$ finishes executing phase 1, $R$ sends phase 2a ``any''
messages to the Fast Paxos acceptors. At this point, the acceptors are free to
vote for the first proposal that they hear from anyone (not just from $R$).

As before, when a BPaxos node $R$ receives a command $a$ from a client, it
sends the command to the ordering service nodes for some instance $R.i$.
Upon receiving $a$, the ordering service node $o_j$ computes its reply $(a,
\deps{a}_j)$. Now, here's the tweak. $o_j$ does not return the reply $(a,
\deps{a}_j)$ directly to $R$. Instead, it proposes the value $(a, \deps{a}_j)$
in instance $R.i$ to $p_j$ (the colocated Fast Paxos acceptor). As we just
described, $p_j$ votes for the first proposal that it hears from anyone, so
$p_j$ votes for the value $(a, \deps{a}_j)$ and relays its phase 2b vote back
to $R$.

If $R$ receives a superquorum of phase 2b votes for the same value $(a,
\deps{a}_{j_1}) = \cdots = (a, \deps{a}_{j_{m}})$ in instance $R.i$, then Fast
Paxos (and hence BPaxos) considers the value chosen. If $R$ does \emph{not}
receive a superquorum of phase 2b votes for the same value, then it computes
$(a, \deps{a})$ where $\deps{a}$ is the union of the dependencies in a quorum
of phase 2b votes. Then, it proposes the value $(a, \deps{a})$ be chosen in
$R.i$ with ballot $1$, a classic ballot, using the variant of Fast Paxos
described at the end of the previous section.

Thus, in the best case, BPaxos can commit a value in one round trip to a
superquorum. Note that this tweak really starts to emphasize the similarities
between BPaxos and Basic EPaxos. Basic EPaxos also commits a value in one round
trip if it hears back from a superquourum of $2f$ nodes that all reply with
identical values of $(\gamma, seq_\gamma, deps_\gamma)$.

To prove that this tweak is correct, we have to prove that \emph{every}
execution of our tweaked protocol is identical to \emph{some} execution of our
untweaked protocol. In particular, we have to prove that if our tweaked
protocol gets some value $(x, \deps{x})$ chosen, then it's possible that our
untweaked protocol could have as well. That is, we have to prove that if $(x,
\deps{x})$ is chosen, then $\deps{x}$ is the union of some quorum of ordering
service responses.

To prove this, we begin by proving the claim $P(i)$ that says if an acceptor
votes for value $(x, \deps{x})$ in round $i$, then either
\begin{itemize}
  \item $i = 0$, or
  \item $\deps{x}$ is the union of a quorum of ordering service replies, or
  \item $x$ is a $\noop$ and $\deps{x} = \emptyset{}$.
\end{itemize}
We prove this by induction. $P(0)$ is trivial. For the general case, we perform
a case analysis on the proposer's logic.
\begin{itemize}
  \item (Case 1)
    $V = \set{(x, \deps{x})}$ and $k \neq 0$. $P(i)$ holds directly from
    $P(k)$.

  \item (Case 2)
    If $k \neq 0$, then $P(i)$ holds directly from $P(k)$. If $k = 0$, then
    it's possible that value $(x, \deps{x})$ was chosen in round $0$ only if
    the proposer receives phase 1b messages from a quorum of acceptors such
    that every acceptor voted for $(x, \deps{x})$ (because superquourum sizes
    are $n$). In this case, the value $(x, \deps{x})$ was an ordering service
    reply on at least a majority of nodes, so $P(i)$ holds.

  \item (Case 3)
    In this case, a proposer always chooses to propose a $\noop$, so $P(i)$
    holds trivially.
\end{itemize}

This claim tells us that if any value is chosen in round $i > 0$, then it is
the union of a quorum of ordering service replies or is a $\noop$. In either
case, this execution is possible in the untweaked algorithm. If a value $(x,
\deps{x})$ is chosen in round $0$, then all ordering service nodes replied with
$\deps{x}$, so it is a union of a majority of ordering service replies.

\subsection{Tweak 3: Coordinated Recovery}
After the previous tweak, a BPaxos node $R$ can commit a command in one round
trip if a superquorum of Paxos acceptors happen to vote for the same set of
dependencies. If $R$ does not receive a superquorum of matching votes, it
performs two additional phases of Paxos to get the command chosen in ballot 1.
Thus, we have one round trip commit latency in the best case and three round
trip commit latency in the next best case (and potentially infinitely more
round trips in the worst case~\cite{fischer1982impossibility}). This tweak, the
final tweak, improves BPaxos to one round trip commit latency in the best case
and two round trip commit latency in the next best case. This matches EPaxos'
commit latencies, save that BPaxos' superquorum sizes are one node larger.

The idea behind this tweak is to leverage a Fast Paxos feature called
coordinated recovery. Coordinated recovery says that if a Fast Paxos leader
owns consecutive ballots $i$ and $i + 1$, and hears phase 2b votes from a
quorum of acceptors for ballot $i$, it can skip phase 1 of ballot $i + 1$ and
proceed directly to phase 2. With this tweak, we make each BPaxos node the
leader of ballots $0$ and $1$. If BPaxos node $R$ receives a quorum of
non-equal votes in ballot $0$, it satisfies the conditions of coordinated
recovery and can initiate phase 2 of ballot 1 directly.

This tweak doesn't affect the correctness of BPaxos because it's merely a
performance optimization for Fast Paxos. The correctness of our protocol does
not depend on whether we use Paxos, Fast Paxos, or Fast Paxos with coordinated
recovery to implement our coordination service.

\section{Fast Bipartisan Paxos}
\section{Faster Bipartisan Paxos}

\bibliographystyle{plain}
\bibliography{references}
\end{document}
