\section{Disaggregating and Scaling}
BPaxos' modular design leads to high throughput in two ways: disaggregation and
scaling.

\subsection{Identifying Bottlenecks}
The throughput of a protocol is determined by its bottleneck. Before we discuss
BPaxos' throughput, we discuss how to identify the bottleneck of a protocol.
Identifying a bottleneck with complete accuracy is hard. Protocol bottlenecks
are affected by many features, including CPU speeds, network bandwidth, message
sizes, workload characteristics, and so on. To make bottleneck analysis
tractable, we make a major simplifying assumption. The assumption is best
explained by way of an example.

{\input{figures/multipaxos_bottleneck.tex}}

Consider the execution of MultiPaxos shown in \figref{MultiPaxosBottleneck} in
which a client proposes a command $x$. The execution involves $N \geq 2f+1$
acceptors. We have annotated each node with the number of messages it sends and
receives in the process of handling $x$. The leader $p_0$ processes $2N+2$
messages, and every acceptor processes $2$ messages. Our major assumption is
that the time required for each node to process command $x$ is directly
proportional to the number of messages that it processes. Thus, the leader
takes time proportional to $2N+2$, and the acceptors take time proportional to
$N$. This means that the leader is the bottleneck, and the protocol's
throughput is directly proportional to $\frac{1}{2N+2}$, the inverse of the
time required by the bottleneck component.

While our assumption is simplistic, we will see in \secref{Evaluation} that
empirically it is accurate enough for us to identify the actual bottleneck of
protocols in practice. Now, we turn our attention to BPaxos. Consider the
execution of BPaxos shown in \figref{BPaxosBottleneck}. We have $N \geq 2f+1$
dependency service nodes, $N$ replicas, $L \geq f+1$ leaders, $L$ proposers,
and $R \geq f+1$ replicas\footnote{We can have a different number of leaders
and proposers, but letting them be equal simplifies the example.}.

{\input{figures/bpaxos_bottleneck.tex}}

Again, we annotate each node with the number of messages it processes to handle
the client's command. The dependency service nodes and acceptors process two
messages each. The replicas process either one or two messages---depending on
whether they are returning a response to the client---for an average of
$1+\frac{1}{R}$. The leaders and proposers process significantly more messages,
$2N+2$ and $2N+R+1$ messages respectively. Thus, the throughput through a
single leader and proposer is proportional to $\frac{1}{2N+R+1}$. With $L$
leaders and proposers executing concurrently, BPaxos' throughput is
proportional to $\frac{L}{2N+R+1}$.

\subsection{Disaggregation}
Many state machine replication protocols pack multiple logical nodes onto a
single physical node. We could do something similar. We could deploy $N=L=R$
dependency service nodes, acceptors, leaders, proposers, and replicas across
$N$ physical ``super nodes'', with one of each component co-located on a single
physical machine. This would reduce the latency of the protocol by two network
delays and open the door for optimizations that could reduce the latency even
further.

However, aggregating logical components together would worsen our bottleneck.
Now, for a given command, a super node would have to process the messages of a
dependency service node, an acceptor, a leader, a proposer, and a replica. With
the bottleneck component processing more messages per command, the throughput
of the protocol decreases. Disaggregating the components allows for pipeline
parallelism in which load is more evenly balanced across the components.

\subsection{Scaling}
Scaling is a classic systems technique that is used to increase throughput of a
system. However, to date, consensus protocols have not been able to take
full advantage of scaling. Conventional wisdom for replication protocols
suggests that we use as few nodes as possible. Returning to
\figref{MultiPaxosBottleneck}, we see this conventional wisdom in action. The
throughput of MultiPaxos is proportional to $\frac{1}{2N+2}$. Adding more
proposers does not do anything, and adding more acceptors (i.e. increasing $N$)
\emph{lowers} the throughput.

BPaxos revises conventional wisdom and notes that while some components are
hard or impossible to scale (e.g., acceptors), other components scale
trivially. Serendipitously, the components that are easy to scale turn out to
be the same components that are a throughput bottleneck.

More specifically, we learned from \figref{BPaxosBottleneck} that BPaxos'
throughput is proportional to $\frac{L}{2N+R+1}$ with the $L$ leaders and
proposers being the bottleneck. To increase BPaxos' throughput, we simply
increase $L$. We can increase the number of leaders and proposers until they
are no longer the bottleneck. This pushes the bottleneck to either the
dependency service nodes, the acceptors, or the replicas. Fortunately, these
nodes only process at most two messages per command. This is equivalent to an
unreplicated state machine which must at least receive and execute a command
and reply with the result. Thus, we have effectively shrunk the throughput
bottleneck to its limit.

Note that we are able to perform this straightforward scaling because BPaxos'
components are modular. When we co-locate components together, $L=N=R$,
and it is impossible for us to increase $L$ (which increases throughput) without
increasing $N$ and $R$ (which decreases throughput). Modularity allows us
to scale each component independently.
