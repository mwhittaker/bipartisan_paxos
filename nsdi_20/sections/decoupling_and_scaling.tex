\section{Decoupling and Scaling}

bpaxos modularity increases its latency
we can see from the figure that we have 8 network delays as opposed to 4 for multip/epaxos/caesar, or 2 for curp/nopaxos. Fortunately, we can trade this latency off for higher throughput. modeularity increases throuhgput in two ways: through decoupling and through scaling

\subsection{Identifying Bottlenecks}
the throughput of a protocol is determined by its bottleneck. before we discuss bpaxos's throughput, we discuss how to identify the bottleneck of a protocol. the true bottleneck of a system depends on a number of factors, cpu speed, network bandwitdh, message sizes, message processing times, etc. thus, performing a perfectly accurate bottleneck analysis is hard, if not impossible. to make identifying a bottleneck tractable, we make a major simplifying assumption. our assumption is best explained via an exmaple.

figure of multipaxos

consider the execution of mlutipaxos shown in figure ??? in which a client propsoes a state machienc command, given n acceptors, we have annotated how many messages each node sends and receives as part of processing x. the leader sends and receives 2n, while the acceptors asend and receive 2. we assume that the time required to process a command x is directly proportional to the number of messages sent and received. thus, the leader takes exactly 2n/2 = n times longer than the acceptors to process x. this means, that the leader is the bottleneck and that the protocols throughput is proportional to 1/2n.

while these assumptions simplistic, we'll see in secref evel that they are good enough for us to accuraetly identify the bottleneck of these protocols in practice.

now we turn to bpaxos. consider the execution of bpaxos shown in figure blah with N=2f+1 dep service nodes, N replicas, L leaders, L proposers, and R replicas. we annotate each node with the number of messages sent and received. dep service nodes and acceptors each process 2 messages, and replicas process either 1 or 2 messages depending on whether they sent (1 + 1/R on average). leaders and propsoers send 2N+2 and 2N+R+1 messages respectively, significantly more messages. thus, the throguhput through a single proposer proportional to 1/(2N+R+1). With L replicas, the throughput is proportional to L/2N+R+1 (or 1/2 if the acceptors if the dep service or acceptors are the bottleneck).

\subsection{Decoupling}
as discussed earlier, many protocols couple together to reduce latency and resource usage. for example, in protocols like epaxos, and caesar, every node is a leader, a proposer, an acceptor, a replica, and a dependency service node (though not described in those terms). We could also colocate all of BPaxos's modules together to reduce latency, as shown in figref blah

figure of colocated

however, doing so mean every node now processes 6n+blah messages instead of 3n. this halves our throughput! by decoupling, every node processes fewer messages and, load is balanced more evenly, and the throughput icnreases.

\subsection{Scaling}
Scaling is a classic systems technique to increase
throughput. However, to date, consensus protocols haven't been able to take
advatnage of scaling. Conventional wisdom for consens states having fewer nodes
is better, more nodes slows things down. ``Using more than $2f+1$ replicas for
$f$ failures is possible but illogical because it requires a larger quorum size
with no additional benefit''~\cite{zhang2018building}.

With BPaxos, we go against conventional wisdom, and note that while some consensus nodes have been hard or impossible to scale, some nodes are easy to scale. moreover, quite fortunately, these scalable nodes happen to be the bottleneck!

we saw that bpaxos throughput is proportional to L/2N+R+1 with the L leaders and L propsoers being th bottleneck. To increase the throughput, we simply increase L and grow the numerator! because every leader is independent from every other elader and every proposer is independent from every other proposer, we can simply add more and more leaders and propsers until they are no longer the bottleneck. this pushes the bottleneck to either the dep service or acceptors, which each process two messages per command. processing two messages per command is equivalent to that of an unreplicated system, so we've pushed the throughput to its boundary

why does this trick work for bpaxos and not other protocols. other protocols, by colocating and tighly couplign components, every leader is ap roposer is a replica, so L = N = R. Now, we cannot increase the numerator without also growing the denomitor. thus, our protocol does not scale
