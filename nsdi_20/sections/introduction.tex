\section{Introduction}
State machine replication protocols like MultiPaxos~\cite{lamport1998part,
lamport2001paxos} and Raft~\cite{ongaro2014search} allow a state machine to be
executed in unison across a number of machines, despite the possibility of
faults. Today, state machine replication is pervasive. Nearly every strongly
consistent distributed system is implemented with some form of state machine
replication~\cite{corbett2013spanner, thomson2012calvin, hunt2010zookeeper,
burrows2006chubby, baker2011megastore, cockroach2019website, cosmos2019website,
tidb2019website, yugabyte2019website}.

MultiPaxos is one of the oldest and one of the most widely used state machine
replication protocols. Despite its popularity, MultiPaxos doesn't have optimal
throughput or optimal latency. In MultiPaxos, \emph{every} command is sent to a
single elected leader. This leader becomes a bottleneck, limiting the
throughput of the protocol. Moreover, when a client sends a command to the
leader, it must wait at least two round trips before receiving a response. This
is twice as long as the theoretical minimum of one round
trip~\cite{lamport2006lower}.

A vast number of state machine replication protocols have been proposed to
address MultiPaxos' suboptimal performance. These protocols use sophisticated
techniques that increase MultiPaxos' throughput, decrease its latency,
or both. For example, techniques like
  deploying multiple leaders~\cite{mao2008mencius, moraru2013there,
  arun2017speeding},
  %
  using flexible quorum sizes~\cite{howard2016flexible, nawab2018dpaxos}, and
  %
  separating the control path from the data path~\cite{biely2012s}
increase MultiPaxos' throughput. Techniques like
  bypassing the leader~\cite{lamport2006fast, ports2015designing, li2016just}
  and
  %
  speculatively executing commands~\cite{ports2015designing, li2016just,
  park2019exploiting}
decrease MultiPaxos' latency. Techniques like
  exploiting commutativity~\cite{lamport2005generalized, moraru2013there,
  arun2017speeding, park2019exploiting}
do both.

Many of these sophisticated protocols try to \emph{simultaneously} increase
throughput and decrease latency, using a combination of the techniques
described in the previous paragraph. For example, NoPaxos ``outperforms both
latency- and throughput-optimized protocols on their respective
metrics''~\cite{li2016just}, and EPaxos achieves ``optimal commit latency
in the wide-area'' while ``achieving high throughput''~\cite{moraru2013there}.

Trying to increase throughput \emph{and} decrease latency is a complex
endeavor. Protocols that aim to improve \emph{both} are forced to implement
multiple of the techniques mentioned above in a single protocol. In isolation,
these techniques are challenging to implement. When put together, they become
even harder. The techniques have to be sewn together in subtle and intricate
ways. These protocols become increasingly complex, with different components
tightly integrated together. Eventually, it becomes difficult to understand any
single piece of a protocol without first having a strong grasp on
the protocol as a whole.

In this paper, we take a different approach. Instead of chasing both throughput
\emph{and} latency, \textbf{we seek \emph{modularity}, which leads to higher
throughput and greater simplicity at the cost of a bit of latency}. We present
Bipartisan Paxos (BPaxos), a state machine replication protocol with a modular
design. BPaxos is composed of a number of disaggregated modules. Each module
can be understood in isolation and composed together in a straightforward way
to form the protocol as a whole. BPaxos' modular design leads to higher
throughput and greater simplicity, though BPaxos does not achieve theoretically
optimal latency.
\TODO[mwhittaker]{%
  Get latency numbers to quantify how much latency we're trading off.
}

\paragraph{High Throughput via Modularity}
Many existing protocols pack a handful of logical processes onto a single
physical process. For example, a single physical process may play the role of a
Paxos proposer, acceptor, learner, \emph{and} replica. This co-location
decreases latency, as messages sent between logical nodes do not have to
traverse the network if the two nodes are physically co-located. Moreover, once
two logical processes have been physically co-located, protocols often perform
further optimizations by merging together the logic of the two components in
clever ways. For example, if an acceptor and a replica both store a set of
proposed values, we can merge the two sets together into a single set if we
co-locate the acceptor and replica.

BPaxos instead draws inspiration from the increasingly popular trend of
disaggregation (e.g., disaggregating storage and compute). Rather than
co-locating and merging different modules together, BPaxos decouples them. This
disaggregation allows us to increase BPaxos' throughput using a simple trick:
scaling. We perform a bottleneck analysis on the protocol's components. Once we
identify the bottleneck component, we simply scale up the component until it is
no longer the bottleneck.

For example, consider a protocol consisting of proposers, acceptors, and
replicas. If we conclude that the proposers are the bottleneck, we can simply
add more proposers. This straightforward scaling is not so easy to do in
tightly coupled protocols in which all components have been aggregated
together. If every node is a proposer, an acceptor, \emph{and} a replica, for
example, then scaling \emph{one} component scales \emph{every} component.

Certain components may speed up when scaled, while others may actually slow
down when scaled.  For example, adding more proposers might speed up the
protocol, but adding more acceptors can lead to larger quorums which can
actually slow down the system. Disaggregation allows us to perform independent
scaling, giving more resources to the components that benefit from scaling,
while giving fewer resources to the components that don't.

\paragraph{Simplicity via Modularity}
It's hard to quantify the ``complexity'' of a protocol, how hard it is for
someone to understand. But, we can look to bugs as a rough proxy of protocol
complexity. Generalized Paxos~\cite{lamport2005generalized} was published in
2005. Seven years later, someone found a bug in one of its
assumptions~\cite{sutra2011fast}.  Zyzyva~\cite{kotla2007zyzzyva}, a Byzantine
replication protocol, was published in 2007. Ten years later, the authors
published a paper noting that the protocol is actually
unsafe~\cite{abraham2017revisiting}. In writing this paper, we discovered bugs
ourselves in two other protocols, EPaxos~\cite{moraru2013there} and
DPaxos~\cite{nawab2018dpaxos}, which we confirmed with the protocols' authors.
These long undiscovered bugs suggest that protocols chasing high throughput and
low latency are often forced to sacrifice simplicity.

BPaxos' modular design, on the other hand, makes the protocol easier to
understand. Each module can be understood and proven correct in isolation,
allowing newcomers to understand the protocol piece by piece. Moreover, some of
the modules implement well-known abstractions for which well-established
protocols already exist. In these cases, BPaxos can leverage existing protocols
instead of reinventing the wheel. For example, BPaxos depends on a consensus
module. Rather than implementing a consensus protocol from scratch and having
to prove it correct (like many other protocols do~\cite{moraru2013there,
arun2017speeding, nawab2018dpaxos}), BPaxos instead uses Paxos off the shelf.
Reusing existing components reduces the learning curve to understand BPaxos,
and it lets us avoid writing complex and error prone proofs of safety.

% In summary, we present the following contributions:
% \begin{itemize}
%   \item
%     We introduce BPaxos, a state machine replication protocol that uses
%     modularity to achieve high throughput and simplicity at the cost of
%     latency.
%   \item
%     We describe how BPaxos' modular, disaggregated design allows us to perform
%     a straightforward form of scaling that yields high throughput without added
%     complexity.
% \end{itemize}
