\section{Bipartisan Paxos}
BPaxos is a modular state machine replication protocol that is both multileader
and generalized. Throughout the paper, we make the standard assumptions that
the network is asynchronous, that state machines are deterministic, and that
machines can fail by crashing but cannot act maliciously. We also assume that
at most $f$ machines can fail for some integer-valued parameter $f$. Throughput
the paper, we omit low-level protocol details involving the re-sending of
dropped messages.

\subsection{Goodbye Logs, Hello Graphs}
MultiPaxos is \emph{not} generalized. It totally orders all commands by
sequencing them into a \emph{log}. BPaxos is generalized, so it ditches the log
and instead partially orders commands into a \emph{directed graph}, like the
ones shown in \figref{ExampleBPaxosExecution}.

BPaxos graphs are completely analogous to MultiPaxos logs. Every MultiPaxos log
entry corresponds to a \defword{vertex} in a BPaxos graph. Every MultiPaxos log
entry holds a command; so does every vertex. Every log entry is uniquely
identified by it's index (e.g., \textcolor{flatred}{$0$}); every vertex is
uniquely identified by a \defword{vertex id} (e.g.,
\textcolor{flatred}{$v_0$}). The one difference between graphs and logs are the
edges. Every BPaxos vertex $v$ has edges to some set of other vertices. These
edges are called the \defword{dependencies} of $v$. Note that we view a
vertex's dependencies as belonging to the vertex, so when we refer to a vertex,
we are also referring to its dependencies. The similarities between MultiPaxos
logs and BPaxos graphs are summarized in \tabref{MultiPaxosVsBPaxos}.

\begin{table}[ht]
  \centering
  \caption{A comparison of MultiPaxos log entries and BPaxos vertices.}
  \tablabel{MultiPaxosVsBPaxos}
  \begin{tabular}{r|l}
    \textbf{BPaxos} & \textbf{MultiPaxos} \\\hline
    graph           & log \\
    vertex          & log entry \\
    vertex id       & index \\
    command         & command \\
    dependencies    & - \\
  \end{tabular}
\end{table}

MultiPaxos grows its \emph{log} over time by repeatedly reaching consensus on
one \emph{log entry} at a time. BPaxos grows its \emph{graph} over time by
repeatedly reaching consensus on one \emph{vertex} (and its dependencies) at a
time. MultiPaxos replicas execute logs in prefix order, making sure not to
execute a command until after executing \emph{all previous commands}. BPaxos
replicas execute graphs in prefix order (i.e. reverse topological order),
making sure not to execute a command until after executing \emph{its
dependencies}.

An example of how BPaxos graphs grow over time and how a BPaxos replica
executes these graphs in shown in \figref{ExampleBPaxosExecution}. As you read
through the figure, note the similarities with
\figref{ExampleMultiPaxosExecution}.
%
First, the command $a \gets 0$ is chosen in vertex $v_0$ with no dependencies
(\figref{ExampleBPaxosExecutionA}).
%
Because the vertex has no dependencies, the replica executes $a \gets 0$
immediately (\figref{ExampleBPaxosExecutionB}).
%
Next, the command $a \gets b$ is chosen in vertex $v_2$ with dependencies on
vertices $v_0$ and $v_1$ (\figref{ExampleBPaxosExecutionC}).
%
$v_2$ depends on $v_1$, but a command has not yet been chosen in $v_1$, so the
replica does \emph{not} yet execute $a \gets b$
(\figref{ExampleBPaxosExecutionD}).
%
Finally, the command $b \gets 0$ is chosen in vertex $v_1$ with no
dependencies (\figref{ExampleBPaxosExecutionE}).
%
Because $b \gets 0$ has no dependencies, the replica executes it immediately.
Moreover, all of $v_2$'s dependencies have been executed, so the replica now
executes $a \gets b$ (\figref{ExampleBPaxosExecutionF}).

{\input{figures/example_bpaxos_execution.tex}}

Before we discuss the mechanisms that BPaxos uses to construct these graphs,
note the following three graph properties.

\paragraph{Vertices are chosen once and for all}
BPaxos reaches consensus on every vertex, so once a vertex has been chosen, it
will never change. It's command won't change, it won't lose dependencies,
and it won't get new dependencies.

\paragraph{Cycles can happen, but aren't a problem}
We'll see in a moment that BPaxos graphs can sometimes be cyclic. These cycles
are a nuisance, but easily handled. Instead of executing graphs in reverse
topological order one \emph{command} at a time, replicas instead execute graphs
in reverse topological order one \emph{strongly connected component} at a time.
The commands within a strongly connected component are executed in an arbitrary
yet deterministic order (e.g., in vertex id order). This is illustrated in
\figref{ExampleBPaxosCycleExecution}.

{\input{figures/example_bpaxos_cycle_execution.tex}}

\paragraph{Conflicting commands depend on each other}
Because BPaxos is generalized, only conflicting commands have to be ordered
with respect to each other. BPaxos ensures this by maintaining the following
invariant:
\begin{invariant}[\defword{dependency invariant}]\invlabel{KeyInvariant}
  If two conflicting commands $x$ and $y$ are chosen in vertices $v_x$ and
  $v_y$, then either $v_x$ depends on $v_y$ or $v_y$ depends on $v_x$ or both.
  That is, there is at least one edge between vertices $v_x$ and $v_y$.
\end{invariant}
Because every conflicting pair of commands has an edge between them, every
replica is guaranteed to execute the commands in the same order.
Non-conflicting commands do not need an edge between them and can be executed
in either order.

\subsection{Protocol Overview}
BPaxos is composed of five modules: a dependency service, a consensus service,
a set of leaders, a set of proposers, and a set of replicas. Here, we give an
overview on how these modules interact by walking through the example execution
shown in \figref{BPaxosOverview}. In the next couple of sections, we discuss
each module in more detail.

1. A client $c$ sends a state machine command $x$ to leader $l_0$. Note that
all of the leaders process commands in parallel and that clients can send
commands to any of them.

2. Upon receiving command $x$, $l_0$ generates a globally unique vertex id
$v_x$ for $x$. It then sends the message $\msg{v_x, x}$ to the dependency
service.

3. Upon receiving message $\msg{v_x, x}$, the dependency service computes a set
of dependencies $\deps{}_x$ for vertex $v_x$. It sends back the message
$\msg{v_x, x, \deps{}_x}$ to $l_0$.

4. $l_0$ forwards $\msg{v_x, x, \deps{}_x}$ to proposer $p_0$.

5. $p_0$ sends the message $\msg{v_x, x, \deps{}_x}$ to the consensus service,
proposing that the value $(x, \deps{}_x)$ be chosen in vertex $v_x$.

6. The consensus service implements one instance of consensus for every vertex.
Upon receiving $\msg{v_x, x, \deps{}_x}$, it chooses the value $(x, \deps{}_x)$
for vertex $v_x$ and notifies $p_0$ with the message $\msg{v_x, x, \deps{}_x}$.
Note that in this example, the consensus service chose the value proposed by
$p_0$. In general, the consensus service may choose some other value if other
proposers are concurrently proposing different values for vertex $v_x$.
However, we'll see later that this can only happen during recovery, and is
therefore not typical.

7. After $p_0$ learns that command $x$ with dependencies $\deps{}_x$ has been
chosen in vertex $v_x$, it notifies the replicas by sending the message
$\msg{v_x, x, \deps{}_x}$.

8. Every replica manages a graph of chosen commands. Upon receiving $\msg{v_x,
x, \deps{}_x}$, the replicas add the vertex $v_x$ to their graph with command
$x$ and dependencies $\deps{}_x$. As described earlier, the replicas execute
the graph in reverse topological order. Once they've executed command $x$,
yielding output $o$, one of the replicas sends back the response to the client
$c$.

{\input{figures/bpaxos_overview.tex}}

\subsection{Dependency Service}
When the dependency service receives a message of the form $\msg{v_x, x}$, it
replies with a set of dependencies $\deps{}_x$ for $v_x$ using the message
$\msg{v_x, x, \deps{}_x}$. The dependency service maintains the following
invariant.

\begin{invariant}[\defword{dependency service invariant}]%
  \invlabel{DepServiceInvariant}
  If the dependency service produces responses $\msg{v_x, x, \deps{}_x}$ and
  $\msg{v_y, y, \deps{}_y}$ for conflicting commands $x$ and $y$, then $v_x \in
  \deps{}_y$ or $v_y \in \deps{}_x$ or both.
\end{invariant}

That is, the dependency service computes dependencies such that conflicting
commands depend on each other. Note that the dependency service invariant
(\invref{DepServiceInvariant}) is very similar to the dependency invariant
(\invref{KeyInvariant}). This is not an accident. Only dependencies computed by
the dependency service can be chosen, so the dependency service invariant
suffices to guarantee that the dependency invariant is maintained.

Concretely, we implement the dependency service with $2f+1$ dependency service
nodes. Every dependency service node maintains a single piece of state,
\textsf{commands}. \textsf{commands} is the set of all the messages that the
dependency service node has received to date. When a dependency service node
receives message $\msg{v_x, x}$ from a leader, it computes
\[
  \deps{} = \setst{y}{\msg{v_y, y} \in \textsf{commands}
                      ~\text{and $x$ and $y$ conflict}}.
\]
It then adds $\msg{v_x, x}$ to \textsf{commands} and sends $\msg{v_x, x,
\deps{}}$ to the leader. When a leader sends a message $\msg{v_x, x}$ to the
dependency service, it sends it to every dependency service node. Upon
receiving $f + 1$ responses, $\set{\msg{v_x, x, \deps{}_1}, \ldots, \msg{v_x, x,
\deps{}_{f+1}}}$, the leader computes the final dependencies as
$\bigcup_{i=1}^{f+1} \deps{}_i$, the union of the computed dependencies.

\begin{theorem}
  The dependency service maintains \invref{DepServiceInvariant}.
\end{theorem}
\begin{proof}
  Assume the dependency service produces responses $\msg{v_x, x, \deps{}_x}$ and
  $\msg{v_y, y, \deps{}_y}$ for conflicting commands $x$ and $y$. We want to
  show that $v_x \in \deps{}_y$ or $v_y \in \deps{}_x$ or both. $\deps{}_x$ is
  the union of dependencies computed by some set $Q_x$ of $f + 1$ dependency
  service nodes. Similarly, $\deps{}_y$ is the union of dependencies computed
  by some set $Q_y$ of $f + 1$ dependency service nodes. Any two sets of $f +
  1$ nodes must intersect ($f+1$ is a majority of $2f+1$). Consider a
  dependency service node $d$ in the intersection of $Q_x$ and $Q_y$. $d$
  received both $\msg{v_x, x}$ and $\msg{v_y, y}$. Without loss of generality,
  assume it received $\msg{v_y, y}$ second. Then, when $d$ received $\msg{v_y,
  y}$, $\msg{v_x, x}$ was already in its \textsf{commands}, so it must have
  included $v_x$ in its computed dependencies for $v_y$. $\deps{}_y$ is a union
  of dependencies that includes the dependencies computed by $d$. Thus, $v_x
  \in \deps{}_y$.
\end{proof}

Note that if the dependency service produces responses $\msg{v_x, x,
\deps{}_x}$ and $\msg{v_y, y, \deps{}_y}$ for conflicting commands $x$ and $y$,
it may include $v_x \in \deps{}_y$ \emph{and} $v_y \in \deps{}_x$. For example,
if dependency service node $d_1$ receives $x$ then $y$ while dependency service
node $d_2$ receives $y$ then $x$, then dependencies formed from $d_1$ and $d_2$
will have $v_x$ and $v_y$ in each other's dependencies. This is the reason why
BPaxos graphs may develop cycles.

\subsection{Leaders}
The dependency

\subsection{Proposers and Consensus Service}
mention you can use paxos with proposers being paxos proposers and this service
implemented as paxos acceptors


\subsection{Recovery}

add pseudocode (including dep service) jk
add protocol diagram

{\input{figures/bpaxos_pseudocode.tex}}
