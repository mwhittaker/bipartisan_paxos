\section{Bipartisan Paxos}
BPaxos is a modular state machine replication protocol that is both multileader
and generalized. Throughout the paper, we make the standard assumptions that
the network is asynchronous, that state machines are deterministic, and that
machines can fail by crashing but cannot act maliciously. We also assume that
at most $f$ machines can fail for some integer-valued parameter $f$. Throughput
the paper, we omit low-level protocol details involving the re-sending of
dropped messages.

\subsection{Goodbye Logs, Hello Graphs}
MultiPaxos is \emph{not} generalized. It totally orders all commands by
sequencing them into a \emph{log}. BPaxos is generalized, so it ditches the log
and instead partially orders commands into a \emph{directed graph}, like the
ones shown in \figref{ExampleBPaxosExecution}.

BPaxos graphs are completely analogous to MultiPaxos logs. Every MultiPaxos log
entry corresponds to a \defword{vertex} in a BPaxos graph. Every MultiPaxos log
entry holds a command; so does every vertex. Every log entry is uniquely
identified by it's index (e.g., \textcolor{flatred}{$0$}); every vertex is
uniquely identified by a \defword{vertex id} (e.g.,
\textcolor{flatred}{$v_0$}). The one difference between graphs and logs are the
edges. Every BPaxos vertex $v$ has edges to some set of other vertices. These
edges are called the \defword{dependencies} of $v$. Note that we view a
vertex's dependencies as belonging to the vertex, so when we refer to a vertex,
we are also referring to its dependencies. The similarities between MultiPaxos
logs and BPaxos graphs are summarized in \tabref{MultiPaxosVsBPaxos}.

\begin{table}[ht]
  \centering
  \caption{A comparison of MultiPaxos log entries and BPaxos vertices.}
  \tablabel{MultiPaxosVsBPaxos}
  \begin{tabular}{r|l}
    \textbf{BPaxos} & \textbf{MultiPaxos} \\\hline
    graph           & log \\
    vertex          & log entry \\
    vertex id       & index \\
    command         & command \\
    dependencies    & - \\
  \end{tabular}
\end{table}

MultiPaxos grows its \emph{log} over time by repeatedly reaching consensus on
one \emph{log entry} at a time. BPaxos grows its \emph{graph} over time by
repeatedly reaching consensus on one \emph{vertex} (and its dependencies) at a
time. MultiPaxos replicas execute logs in prefix order, making sure not to
execute a command until after executing \emph{all previous commands}. BPaxos
replicas execute graphs in prefix order (i.e. reverse topological order),
making sure not to execute a command until after executing \emph{its
dependencies}.

An example of how BPaxos graphs grow over time and how a BPaxos replica
executes these graphs in shown in \figref{ExampleBPaxosExecution}. As you read
through the figure, note the similarities with
\figref{ExampleMultiPaxosExecution}.
%
First, the command $a \gets 0$ is chosen in vertex $v_0$ with no dependencies
(\figref{ExampleBPaxosExecutionA}).
%
Because the vertex has no dependencies, the replica executes $a \gets 0$
immediately (\figref{ExampleBPaxosExecutionB}).
%
Next, the command $a \gets b$ is chosen in vertex $v_2$ with dependencies on
vertices $v_0$ and $v_1$ (\figref{ExampleBPaxosExecutionC}).
%
$v_2$ depends on $v_1$, but a command has not yet been chosen in $v_1$, so the
replica does \emph{not} yet execute $a \gets b$
(\figref{ExampleBPaxosExecutionD}).
%
Finally, the command $b \gets 0$ is chosen in vertex $v_1$ with no
dependencies (\figref{ExampleBPaxosExecutionE}).
%
Because $b \gets 0$ has no dependencies, the replica executes it immediately.
Moreover, all of $v_2$'s dependencies have been executed, so the replica now
executes $a \gets b$ (\figref{ExampleBPaxosExecutionF}).

{\input{figures/example_bpaxos_execution.tex}}

Before we discuss the mechanisms that BPaxos uses to construct these graphs,
note the following three graph properties.

\paragraph{Vertices are chosen once and for all}
BPaxos reaches consensus on every vertex, so once a vertex has been chosen, it
will never change. It's command won't change, it won't lose dependencies,
and it won't get new dependencies.

\paragraph{Cycles can happen, but aren't a problem}
We'll see in a moment that BPaxos graphs can sometimes be cyclic. These cycles
are a nuisance, but easily handled. Instead of executing graphs in reverse
topological order one \emph{command} at a time, replicas instead execute graphs
in reverse topological order one \emph{strongly connected component} at a time.
The commands within a strongly connected component are executed in an arbitrary
yet deterministic order (e.g., in vertex id order). This is illustrated in
\figref{ExampleBPaxosCycleExecution}.

{\input{figures/example_bpaxos_cycle_execution.tex}}

\paragraph{Conflicting commands depend on each other}
Because BPaxos is generalized, only conflicting commands have to be ordered
with respect to each other. BPaxos ensures this by maintaining the following
invariant:
\begin{invariant}[\defword{dependency invariant}]\invlabel{KeyInvariant}
  If two conflicting commands $x$ and $y$ are chosen in vertices $v_x$ and
  $v_y$, then either $v_x$ depends on $v_y$ or $v_y$ depends on $v_x$ or both.
  That is, there is at least one edge between vertices $v_x$ and $v_y$.
\end{invariant}
If two commands have an edge between them, every replica executes them in the
same order.  The dependency invariant ensures that every conflicting pair of
commands has an edge between them, ensuring that all conflicting commands are
executed in the same order. Non-conflicting commands do not need an edge
between them and can be executed in any order.

\subsection{Protocol Overview}
BPaxos is composed of five modules: a dependency service, a consensus service,
a set of leaders, a set of proposers, and a set of replicas. Here, we give an
overview on how these modules interact by walking through the example execution
shown in \figref{BPaxosOverview}. In the next couple of sections, we discuss
each module in more detail.

1. A client $c$ sends a state machine command $x$ to leader $l_0$. Note that
all of the leaders process commands in parallel and that clients can send
commands to any of them.

2. Upon receiving command $x$, $l_0$ generates a globally unique vertex id
$v_x$ for $x$. It then sends the message $\msg{v_x, x}$ to the dependency
service.

3. Upon receiving message $\msg{v_x, x}$, the dependency service computes a set
of dependencies $\deps{}_x$ for vertex $v_x$. Later, we'll see exactly how the
dependency service computes dependencies. For now, we overlook the details. The
dependency service then sends back the message $\msg{v_x, x, \deps{}_x}$ to
$l_0$.

4. $l_0$ forwards $\msg{v_x, x, \deps{}_x}$ to proposer $p_0$.

5. $p_0$ sends the message $\msg{v_x, x, \deps{}_x}$ to the consensus service,
proposing that the value $(x, \deps{}_x)$ be chosen in vertex $v_x$.

6. The consensus service implements one instance of consensus for every vertex.
Upon receiving $\msg{v_x, x, \deps{}_x}$, it chooses the value $(x, \deps{}_x)$
for vertex $v_x$ and notifies $p_0$ with the message $\msg{v_x, x, \deps{}_x}$.
Note that in this example, the consensus service chose the value proposed by
$p_0$. In general, the consensus service may choose some other value if other
proposers are concurrently proposing different values for vertex $v_x$.
However, we'll see later that this can only happen during recovery, and is
therefore not typical.

7. After $p_0$ learns that command $x$ with dependencies $\deps{}_x$ has been
chosen in vertex $v_x$, it notifies the replicas by broadcasting the message
$\msg{v_x, x, \deps{}_x}$.

8. Every replica manages a graph of chosen commands, as described in the
previous subsection. Upon receiving $\msg{v_x, x, \deps{}_x}$, the replicas add
the vertex $v_x$ to their graph with command $x$ and dependencies $\deps{}_x$.
As described earlier, the replicas execute the graph in reverse topological
order. Once they've executed command $x$, yielding output $o$, one of the
replicas sends back the response to the client $c$.

Pseudocode for BPaxos is given in \figref{BPaxosPseudocode}, and a TLA+
specification of BPaxos is given in \appref{TlaSpec}. We now detail each BPaxos
module.

{\input{figures/bpaxos_overview.tex}}
{\input{figures/bpaxos_pseudocode.tex}}

\subsection{Dependency Service}
When the dependency service receives a message of the form $\msg{v_x, x}$, it
replies with a set of dependencies $\deps{}_x$ for $v_x$ using the message
$\msg{v_x, x, \deps{}_x}$.

Concretely, we implement the dependency service with $2f+1$ dependency service
nodes. Every dependency service node maintains a single piece of state,
\textsf{commands}. \textsf{commands} is the set of all the messages that the
dependency service node has received to date. When a dependency service node
receives message $\msg{v_x, x}$ from a leader, it computes the dependencies of
$v_x$ as the set of all vertices $v_y$ in $\textsf{commands}$ that contain a
command that conflicts with $x$:
\[
  \deps{} = \setst{v_y}{\msg{v_y, y} \in \textsf{commands}
                        ~\text{and $x$ and $y$ conflict}}.
\]
It then adds $\msg{v_x, x}$ to \textsf{commands} and sends $\msg{v_x, x,
\deps{}}$ back to the leader. When a leader sends a message $\msg{v_x, x}$ to
the dependency service, it sends it to every dependency service node. Upon
receiving $f + 1$ responses, $\set{\msg{v_x, x, \deps{}_1}, \ldots, \msg{v_x,
x, \deps{}_{f+1}}}$, the leader computes the final dependencies as
$\bigcup_{i=1}^{f+1} \deps{}_i$, the union of the computed dependencies.

The dependency service maintains the following invariant.

\begin{invariant}[\defword{dependency service invariant}]%
  \invlabel{DepServiceInvariant}
  If the dependency service produces responses $\msg{v_x, x, \deps{}_x}$ and
  $\msg{v_y, y, \deps{}_y}$ for conflicting commands $x$ and $y$, then $v_x \in
  \deps{}_y$ or $v_y \in \deps{}_x$ or both.
\end{invariant}

That is, the dependency service computes dependencies such that conflicting
commands depend on each other. Note that the dependency service invariant
(\invref{DepServiceInvariant}) is very similar to the dependency invariant
(\invref{KeyInvariant}). This is not an accident. Only dependencies computed by
the dependency service can be chosen, so the dependency service invariant
suffices to guarantee that the dependency invariant is maintained.

\begin{theorem}
  The dependency service maintains \invref{DepServiceInvariant}.
\end{theorem}
\begin{proof}
  Assume the dependency service produces responses $\msg{v_x, x, \deps{}_x}$ and
  $\msg{v_y, y, \deps{}_y}$ for conflicting commands $x$ and $y$. We want to
  show that $v_x \in \deps{}_y$ or $v_y \in \deps{}_x$ or both. $\deps{}_x$ is
  the union of dependencies computed by some set $Q_x$ of $f + 1$ dependency
  service nodes. Similarly, $\deps{}_y$ is the union of dependencies computed
  by some set $Q_y$ of $f + 1$ dependency service nodes. Any two sets of $f +
  1$ nodes must intersect ($f+1$ is a majority of $2f+1$). Consider a
  dependency service node $d$ in the intersection of $Q_x$ and $Q_y$. $d$
  received both $\msg{v_x, x}$ and $\msg{v_y, y}$. Without loss of generality,
  assume it received $\msg{v_y, y}$ second. Then, when $d$ received $\msg{v_y,
  y}$, $\msg{v_x, x}$ was already in its \textsf{commands}, so it must have
  included $v_x$ in its computed dependencies for $v_y$. $\deps{}_y$ is a union
  of dependencies that includes the dependencies computed by $d$. Thus, $v_x
  \in \deps{}_y$. This is illustrated below.

  {\input{figures/depservice_proof.tex}}
\end{proof}

Note that if the dependency service produces responses $\msg{v_x, x,
\deps{}_x}$ and $\msg{v_y, y, \deps{}_y}$ for conflicting commands $x$ and $y$,
it may include $v_x \in \deps{}_y$ \emph{and} $v_y \in \deps{}_x$. For example,
if dependency service node $d_1$ receives $x$ then $y$ while dependency service
node $d_2$ receives $y$ then $x$, then dependencies formed from $d_1$ and $d_2$
will have $v_x$ and $v_y$ in each other's dependencies. This is the reason why
BPaxos graphs may develop cycles.

Also note that the dependency service is an independent module within BPaxos.
The dependency service is unaware of consensus, or BPaxos graphs, or state
machines, or any other detail outside of the dependency service. The dependency
service can be completely understood in isolation.

\subsection{Leaders}
When a leader receives a command $x$ from a client, it assigns $x$ a globally
unique vertex id $v_x$. The mechanism by which leaders generate unique ids is
unimportant. You can use any mechanism you'd like as long as ids are globally
unique. In our implementation, a vertex id is a tuple of the leader's index and
a monotonically increasing id beginning at $0$. For example, leader $2$
generates vertex ids $(2, 0), (2, 1), (2, 2)$, and so on.

After generating a vertex id $v_x$, the leader sends $\msg{v_x, x}$ to all
dependency service nodes, aggregates the dependencies from $f+1$ of them, and
forwards the dependencies to a proposer.

\subsection{Proposers and Consensus Service}
When a proposer receives a message $\msg{v_x, x, \deps{}_x}$, it proposes to
the consensus service that the value $(x, \deps{}_x)$ be chosen for vertex
$v_x$. The consensus service implements one instance of consensus for every
vertex, and eventually informs the proposer of the value $(x', \deps{}_x')$
that was chosen for vertex $v_x$. In the normal case, $(x', \deps{}_x')$ is
equal to $(x, \deps{}_x)$, but the consensus service is free to choose any
value proposed for vertex $v_x$.

You can implement the consensus service with any consensus protocol that you'd
like. In our implementation of BPaxos, BPaxos proposers are Paxos proposers,
and the consensus service is implemented as $2f+1$ Paxos acceptors. We
implement Paxos with the standard optimization that phase 1 of the protocol can
be skipped in round $0$ (a.k.a.\ ballot $0$). Doing so, and partitioning vertex
ids uniformly across proposers, the proposers can get a value chosen in one
round trip to the acceptors (in the common case). This optimization is very
similar to the one done in MultiPaxos.

Because the consensus service is an independent module, we can use the Paxos
protocol unmodified. We do not have to special purpose Paxos to fit the needs
of BPaxos. This reduces the complexity of BPaxos, as we avoid having to reason
about a consensus protocol written from scratch.

\subsection{Replicas}
Every BPaxos replica maintains a BPaxos graph and an instance of a state
machine. All state machine begin in the same initial state. Upon receiving a
message $\msg{v_x, x, \deps{}_x}$ from a proposer, a replica adds vertex $v_x$
to its graph with command $x$ and with edges to $\deps{}_x$. As discussed
earlier, the replicas execute their graphs in reverse topological order, one
component at a time. When a replica is ready to execute a command $x$, it
passes it to the state machine. The state machine transitions to a new state
and produces some output $o$. The replicas then return $o$ to the client that
initially proposed $x$. As an optimization in our BPaxos implementation, only
one replica returns $o$ to the client, though it is safe for every replica to
do so. In particular, given $n$ replicas, $r_i$ returns outputs to clients for
vertices $v_x$ where $\text{hash}(v_x) \% n = i$.

\subsection{Fault Tolerance and Recovery}
BPaxos can tolerate up to $f$ failures. By deploying $f+1$ leaders, proposers,
and replicas, BPaxos guarantees that at least one of each is operational after
$f$ failures. The dependency service deploys $2f+1$ dependency service nodes,
ensuring that at a quorum of $f+1$ nodes is available despite $f$ failures. The
consensus service tolerates $f$ failures by assumption. In our implementation,
we use $2f+1$ Paxos acceptors, as is standard.

However, despite this, failures can still lead to liveness violations if we're
not careful. A replica executes vertex $v_x$ only after it has executed $v_x$'s
dependencies. If one of $v_x$'s dependencies has not yet been chosen, then the
execution of $v_x$ is delayed. For example, in \figref{ExampleBPaxosExecution},
the execution of $v_2$ is delayed until after $v_1$ has been chosen and
executed.

If a vertex $v_x$ depends on a vertex $v_y$ that remains forever unchosen, then
$v_x$ is never executed! This situation is rare, but possible in the event of
failures. For example, if two leaders $l_x$ and $l_y$ concurrently send
commands $x$ and $y$ in vertices $v_x$ and $v_y$ to the dependency service, and
if $l_y$ then crashes, it's possible that $v_x$ gets chosen with a dependency
on $v_y$, but $v_y$ remains forever unchosen.

Dealing with these sorts of failure scenarios to ensure that every command
eventually gets chosen is called \defword{recovery}. Every state machine
replication protocol has to implement some form of recovery, and for many
protocols (though not all protocols), recovery is its most complicated part.

\newcommand{\noop}{\text{noop}}
Fortunately, BPaxos' modularity leads to a very simple recovery protocol. When
a replica notices that a vertex $v_x$ has been blocked waiting for another
vertex $v_y$ for more than some configurable amount of time, the replica
contacts the consensus service and proposes that the command $\noop$ be chosen
for vertex $v_y$ with no dependencies. $\noop$ is a special ``dud command''
that does not affect the state machine and does not conflict with any other
command. Eventually, the consensus protocol returns the chosen value to the
replica, and the execution of $v_x$ can proceed.
