\section{Bipartisan Paxos}
BPaxos is a modular state machine replication protocol that is both multileader
and generalized. Throughout the paper, we make the standard assumptions that
the network is asynchronous, that state machines are deterministic, and that
machines can fail by crashing but cannot act maliciously. We also assume that
at most $f$ machines can fail for some integer-valued parameter $f$. Throughout
the paper, we omit low-level protocol details involving the re-sending of
dropped messages.

\subsection{BPaxos Command Execution}
MultiPaxos is \emph{not} generalized. It totally orders all commands by
sequencing them into a \emph{log}. BPaxos is generalized, so it ditches the log
and instead partially orders commands into a \emph{directed graph}, like the
ones shown in \figref{ExampleBPaxosExecution}.

BPaxos graphs are completely analogous to MultiPaxos logs. Every MultiPaxos log
entry corresponds to a \defword{vertex} in a BPaxos graph. Every MultiPaxos log
entry holds a command; so does every vertex. Every log entry is uniquely
identified by its index (e.g., \textcolor{flatred}{$0$}); every vertex is
uniquely identified by a \defword{vertex id} (e.g.,
\textcolor{flatred}{$v_0$}). The one difference between graphs and logs are the
edges. Every BPaxos vertex $v$ has edges to some set of other vertices. These
edges are called the \defword{dependencies} of $v$. Note that we view a
vertex's dependencies as belonging to the vertex, so when we refer to a vertex,
we are also referring to its dependencies. The similarities between MultiPaxos
logs and BPaxos graphs are summarized in \tabref{MultiPaxosVsBPaxos}.

\begin{table}[ht]
  \centering
  \caption{A comparison of MultiPaxos log entries and BPaxos vertices.}
  \tablabel{MultiPaxosVsBPaxos}
  \begin{tabular}{r|l}
    \textbf{BPaxos} & \textbf{MultiPaxos} \\\hline
    graph           & log \\
    vertex          & log entry \\
    vertex id       & index \\
    command         & command \\
    dependencies    & - \\
  \end{tabular}
\end{table}

MultiPaxos grows its \emph{log} over time by repeatedly reaching consensus on
one \emph{log entry} at a time. BPaxos grows its \emph{graph} over time by
repeatedly reaching consensus on one \emph{vertex} (and its dependencies) at a
time. MultiPaxos replicas execute logs in prefix order, making sure not to
execute a command until after executing \emph{all previous commands}. BPaxos
replicas execute graphs in prefix order (i.e. reverse topological order),
making sure not to execute a command until after executing \emph{its
dependencies}.

An example of how BPaxos graphs grow over time and how a BPaxos replica
executes these graphs in shown in \figref{ExampleBPaxosExecution}. As you read
through the figure, note the similarities with
\figref{ExampleMultiPaxosExecution}.
%
First, the command $a \gets 0$ is chosen in vertex $v_0$ with no dependencies
(\figref{ExampleBPaxosExecutionA}).
%
Because the vertex has no dependencies, the replica executes $a \gets 0$
immediately (\figref{ExampleBPaxosExecutionB}).
%
Next, the command $a \gets b$ is chosen in vertex $v_2$ with dependencies on
vertices $v_0$ and $v_1$ (\figref{ExampleBPaxosExecutionC}).
%
$v_2$ depends on $v_1$, but a command has not yet been chosen in $v_1$, so the
replica does \emph{not} yet execute $a \gets b$
(\figref{ExampleBPaxosExecutionD}).
%
Finally, the command $b \gets 0$ is chosen in vertex $v_1$ with no
dependencies (\figref{ExampleBPaxosExecutionE}).
%
Because $b \gets 0$ has no dependencies, the replica executes it immediately.
Moreover, all of $v_2$'s dependencies have been executed, so the replica now
executes $a \gets b$ (\figref{ExampleBPaxosExecutionF}).

{\input{figures/example_bpaxos_execution.tex}}

Before we discuss the mechanisms that BPaxos uses to construct these graphs,
note the following three graph properties.

\paragraph{Vertices are chosen once and for all.}
BPaxos reaches consensus on every vertex, so once a vertex has been chosen, it
will never change. Its command will not change, it will not lose dependencies,
and it will not get new dependencies.

\paragraph{Cycles can happen, but are not a problem.}
We will see in a moment that BPaxos graphs can sometimes be cyclic. These cycles
are a nuisance, but easily handled. Instead of executing graphs in reverse
topological order one \emph{command} at a time, replicas instead execute graphs
in reverse topological order one \emph{strongly connected component} at a time.
The commands within a strongly connected component are executed in an arbitrary
yet deterministic order (e.g., in vertex id order). This is illustrated in
\figref{ExampleBPaxosCycleExecution}.

{\input{figures/example_bpaxos_cycle_execution.tex}}

\paragraph{Conflicting commands depend on each other.}
Because BPaxos is generalized, only conflicting commands have to be ordered
with respect to each other. BPaxos ensures this by maintaining the following
invariant:
\begin{invariant}[\defword{dependency invariant}]\invlabel{KeyInvariant}
  If two conflicting commands $x$ and $y$ are chosen in vertices $v_x$ and
  $v_y$, then either $v_x$ depends on $v_y$ or $v_y$ depends on $v_x$ or both.
  That is, there is at least one edge between vertices $v_x$ and $v_y$.
\end{invariant}
If two commands have an edge between them, every replica executes them in the
same order.  The dependency invariant ensures that every conflicting pair of
commands has an edge between them, ensuring that all conflicting commands are
executed in the same order. Non-conflicting commands do not need an edge
between them and can be executed in any order.

\subsection{Protocol Overview}
BPaxos is composed of five modules: a dependency service, a consensus service,
a set of leaders, a set of proposers, and a set of replicas. Here, we give an
overview on how these modules interact by walking through the example execution
shown in \figref{BPaxosOverview}. In the next couple of sections, we discuss
each module in more detail.

1. A client $c$ sends a state machine command $x$ to leader $l_0$. Note that
all of the leaders process commands in parallel and that clients can send
commands to any of them.

2. Upon receiving command $x$, $l_0$ generates a globally unique vertex id
$v_x$ for $x$. It then sends the message $\msg{v_x, x}$ to the dependency
service.

3. Upon receiving message $\msg{v_x, x}$, the dependency service computes a set
of dependencies $\deps{}_x$ for vertex $v_x$. Later, we will see exactly how the
dependency service computes dependencies. For now, we overlook the details. The
dependency service then sends back the message $\msg{v_x, x, \deps{}_x}$ to
$l_0$.

4. $l_0$ forwards $\msg{v_x, x, \deps{}_x}$ to proposer $p_0$.

5. $p_0$ sends the message $\msg{v_x, x, \deps{}_x}$ to the consensus service,
proposing that the value $(x, \deps{}_x)$ be chosen in vertex $v_x$.

6. The consensus service implements one instance of consensus for every vertex.
Upon receiving $\msg{v_x, x, \deps{}_x}$, it chooses the value $(x, \deps{}_x)$
for vertex $v_x$ and notifies $p_0$ with the message $\msg{v_x, x, \deps{}_x}$.
Note that in this example, the consensus service chose the value proposed by
$p_0$. In general, the consensus service may choose some other value if other
proposers are concurrently proposing different values for vertex $v_x$.
However, we will see later that this can only happen during recovery and is
therefore not typical.

7. After $p_0$ learns that command $x$ with dependencies $\deps{}_x$ has been
chosen in vertex $v_x$, it notifies the replicas by broadcasting the message
$\msg{v_x, x, \deps{}_x}$.

8. Every replica manages a graph of chosen commands, as described in the
previous subsection. Upon receiving $\msg{v_x, x, \deps{}_x}$, a replica adds
the vertex $v_x$ to ts graph with command $x$ and dependencies $\deps{}_x$.
As described earlier, the replicas execute their graphs in reverse topological
order. Once they have executed command $x$, yielding output $o$, one of the
replicas sends back the response to the client $c$. Given $r$ replicas, replica
$i$ sends back the response where $i = \text{hash}(v_x) \% r$ for some hash
function.

Pseudocode for BPaxos is given in \figref{BPaxosPseudocode}, and a TLA+
specification of BPaxos is given in \appref{TlaSpec}. We now detail each BPaxos
module. In the next section, we discuss why the dependency service, consensus
service, and replicas don't scale and why the leaders and proposers do.

{\input{figures/bpaxos_overview.tex}}
{\input{figures/bpaxos_pseudocode.tex}}

\subsection{Dependency Service}
When the dependency service receives a message of the form $\msg{v_x, x}$, it
replies with a set of dependencies $\deps{}_x$ for $v_x$ using the message
$\msg{v_x, x, \deps{}_x}$.

Concretely, we implement the dependency service with $2f+1$ dependency service
nodes. Every dependency service node maintains a single piece of state,
\textsf{commands}. \textsf{commands} is the set of all the messages that the
dependency service node has received to date. When a dependency service node
receives message $\msg{v_x, x}$ from a leader, it computes the dependencies of
$v_x$ as the set of all vertices $v_y$ in $\textsf{commands}$ that contain a
command that conflicts with $x$:
\[
  \deps{} = \setst{v_y}{\msg{v_y, y} \in \textsf{commands}
                        ~\text{and $x$ and $y$ conflict}}.
\]
It then adds $\msg{v_x, x}$ to \textsf{commands} and sends $\msg{v_x, x,
\deps{}}$ back to the leader. When a leader sends a message $\msg{v_x, x}$ to
the dependency service, it sends it to every dependency service node. Upon
receiving $f + 1$ responses, $\set{\msg{v_x, x, \deps{}_1}, \ldots, \msg{v_x,
x, \deps{}_{f+1}}}$, the leader computes the final dependencies as
$\bigcup_{i=1}^{f+1} \deps{}_i$, the union of the computed dependencies.

The dependency service maintains the following invariant.

\begin{invariant}[\defword{dependency service invariant}]%
  \invlabel{DepServiceInvariant}
  If the dependency service produces responses $\msg{v_x, x, \deps{}_x}$ and
  $\msg{v_y, y, \deps{}_y}$ for conflicting commands $x$ and $y$, then $v_x \in
  \deps{}_y$ or $v_y \in \deps{}_x$ or both.
\end{invariant}

That is, the dependency service computes dependencies such that conflicting
commands depend on each other. Note that the dependency service invariant
(\invref{DepServiceInvariant}) is very similar to the dependency invariant
(\invref{KeyInvariant}). This is not an accident. Only dependencies computed by
the dependency service can be chosen, so the dependency service invariant
suffices to guarantee that the dependency invariant is maintained.

\begin{theorem}\thmlabel{DepServiceInvariant}
  The dependency service maintains \invref{DepServiceInvariant}.
\end{theorem}
\begin{proof}
  Assume the dependency service produces responses $\msg{v_x, x, \deps{}_x}$ and
  $\msg{v_y, y, \deps{}_y}$ for conflicting commands $x$ and $y$. We want to
  show that $v_x \in \deps{}_y$ or $v_y \in \deps{}_x$ or both. $\deps{}_x$ is
  the union of dependencies computed by some set $Q_x$ of $f + 1$ dependency
  service nodes. Similarly, $\deps{}_y$ is the union of dependencies computed
  by some set $Q_y$ of $f + 1$ dependency service nodes. Any two sets of $f +
  1$ nodes must intersect ($f+1$ is a majority of $2f+1$). Consider a
  dependency service node $d$ in the intersection of $Q_x$ and $Q_y$. $d$
  received both $\msg{v_x, x}$ and $\msg{v_y, y}$. Without loss of generality,
  assume it received $\msg{v_y, y}$ second. Then, when $d$ received $\msg{v_y,
  y}$, $\msg{v_x, x}$ was already in its \textsf{commands}, so it must have
  included $v_x$ in its computed dependencies for $v_y$. $\deps{}_y$ is a union
  of dependencies that includes the dependencies computed by $d$. Thus, $v_x
  \in \deps{}_y$. This is illustrated in \figref{DepServiceProof}.
\end{proof}

{\input{figures/depservice_proof.tex}}

Note that if the dependency service produces responses $\msg{v_x, x,
\deps{}_x}$ and $\msg{v_y, y, \deps{}_y}$ for conflicting commands $x$ and $y$,
it may include $v_x \in \deps{}_y$ \emph{and} $v_y \in \deps{}_x$. For example,
if dependency service node $d_1$ receives $x$ then $y$ while dependency service
node $d_2$ receives $y$ then $x$, then dependencies formed from $d_1$ and $d_2$
will have $v_x$ and $v_y$ in each other's dependencies. This is the reason why
BPaxos graphs may develop cycles.

Also note that the dependency service is an independent module within BPaxos.
The dependency service is unaware of consensus, or BPaxos graphs, or state
machines, or any other detail outside of the dependency service. The dependency
service can be completely understood in isolation. In contrast, dependency
computation in EPaxos and Caesar is tightly coupled with the rest of the
protocol. For example, in Caesar, every command is assigned a timestamp. If a
node receives two commands out of timestamp order, it must first wait to see if
the higher timestamp command gets chosen with a dependency on the lower
timstamp command before it is able to compute the lower timestamp command's
dependencies. This coupling prevents us from understanding dependency
computation in isolation.

\subsection{Leaders}
When a leader receives a command $x$ from a client, it assigns $x$ a globally
unique vertex id $v_x$. The mechanism by which leaders generate unique ids is
unimportant. You can use any mechanism you would like as long as ids are globally
unique. In our implementation, a vertex id is a tuple of the leader's index and
a monotonically increasing id beginning at $0$. For example, leader $2$
generates vertex ids $(2, 0), (2, 1), (2, 2)$, and so on.

After generating a vertex id $v_x$, the leader sends $\msg{v_x, x}$ to all
dependency service nodes, aggregates the dependencies from $f+1$ of them, and
forwards the dependencies to a proposer.

\subsection{Proposers and Consensus Service}
When a proposer receives a message $\msg{v_x, x, \deps{}_x}$, it proposes to
the consensus service that the value $(x, \deps{}_x)$ be chosen for vertex
$v_x$. The consensus service implements one instance of consensus for every
vertex, and eventually informs the proposer of the value $(x', \deps{}_x')$
that was chosen for vertex $v_x$. In the normal case, $(x', \deps{}_x')$ is
equal to $(x, \deps{}_x)$, but the consensus service is free to choose any
value proposed for vertex $v_x$.

You can implement the consensus service with any consensus protocol that you would
like. In our implementation of BPaxos, BPaxos proposers are Paxos proposers,
and the consensus service is implemented as $2f+1$ Paxos acceptors. We
implement Paxos with the standard optimization that phase 1 of the protocol can
be skipped in round $0$ (a.k.a.\ ballot $0$). Doing so, and partitioning vertex
ids uniformly across proposers, the proposers can get a value chosen in one
round trip to the acceptors (in the common case). This optimization is very
similar to the one done in MultiPaxos.

Again, note that the consensus service is an independent module that we can
understand in isolation. The consensus service implements consensus, and that
is it. It is unaware of dependencies, graphs, or any other detail of the
protocol. Moreover, note that the consensus service is not specialized at all
to BPaxos. We are able to use the Paxos protocol without modification. This
lets us avoid having to prove a specialized implementation of consensus
correct.

\subsection{Replicas}
Every BPaxos replica maintains a BPaxos graph and an instance of a state
machine. Every state machine begins in the same initial state. Upon receiving a
message $\msg{v_x, x, \deps{}_x}$ from a proposer, a replica adds vertex $v_x$
to its graph with command $x$ and with edges to $\deps{}_x$. As discussed
earlier, the replicas execute their graphs in reverse topological order, one
component at a time. When a replica is ready to execute a command $x$, it
passes it to the state machine. The state machine transitions to a new state
and produces some output $o$. One replica then returns $o$ to the client that
initially proposed $x$. In particular, given $n$ replicas, $r_i$ returns
outputs to clients for vertices $v_x$ where $\text{hash}(v_x) \% n = i$.

\subsection{Summary}
In summary, BPaxos is composed of five modules: leaders, dependency service
nodes, proposers, a consensus service, and replicas. Clients propose commands;
leaders assign unique ids to commands; the dependency service computes
dependencies (ensuring that conflicting commands depend on each other); the
proposers and consensus service reach consensus on every vertex; and replicas
execute commands.

\subsection{Fault Tolerance and Recovery}
BPaxos can tolerate up to $f$ failures. By deploying $f+1$ leaders, proposers,
and replicas, BPaxos guarantees that at least one of each is operational after
$f$ failures. The dependency service deploys $2f+1$ dependency service nodes,
ensuring that at a quorum of $f+1$ nodes is available despite $f$ failures. The
consensus service tolerates $f$ failures by assumption. In our implementation,
we use $2f+1$ Paxos acceptors, as is standard.

However, despite this, failures can still lead to liveness violations if we are
not careful. A replica executes vertex $v_x$ only after it has executed $v_x$'s
dependencies. If one of $v_x$'s dependencies has not yet been chosen, then the
execution of $v_x$ is delayed. For example, in \figref{ExampleBPaxosExecution},
the execution of $v_2$ is delayed until after $v_1$ has been chosen and
executed.

If a vertex $v_x$ depends on a vertex $v_y$ that remains forever unchosen, then
$v_x$ is never executed. This situation is rare, but possible in the event of
failures. For example, if two leaders $l_x$ and $l_y$ concurrently send
commands $x$ and $y$ in vertices $v_x$ and $v_y$ to the dependency service, and
if $l_y$ then crashes, it is possible that $v_x$ gets chosen with a dependency
on $v_y$, but $v_y$ remains forever unchosen.

Dealing with these sorts of failure scenarios to ensure that every command
eventually gets chosen is called \defword{recovery}. Every state machine
replication protocol has to implement some form of recovery, and for many
protocols (though not all protocols), recovery is its most complicated part.

\newcommand{\noop}{\text{noop}}
Fortunately, BPaxos' modularity leads to a very simple recovery protocol. When
a replica notices that a vertex $v_x$ has been blocked waiting for another
vertex $v_y$ for more than some configurable amount of time, the replica
contacts the consensus service and proposes that a no-operation command $\noop$
be chosen for vertex $v_y$ with no dependencies. $\noop$ is a special command
that does not affect the state machine and does not conflict with any other
command. Eventually, the consensus protocol returns the chosen value to the
replica, and the execution of $v_x$ can proceed.

% \subsection{Discussion}\seclabel{Discussion}
% It is difficult to precisely measure the ``complexity'' of a protocol, how hard
% it is for someone to understand it. However, in \secref{Discussion}, we present a number of
%
% we can look at a couple of anecdotes that corroborate our claims of simplicity.
% For example, BPaxos' TLA+ specification (\appref{TlaSpec}) is 142 lines long. The TLA+
% specifications of two other state of the art protocols, EPaxos and Caesar, are
% 583 and 674 lines long respectively~\cite{moraru2013there, arun2017speeding}. While the length of a protocol's TLA+ specification is an imperfect metric of simplicity, the 4x reduction in length represents
%
% % Second, \textbf{they are complicated}. MultiPaxos is notoriously difficult to
% % understand, and these sophisticated protocols are even more complex. For
% % evidence, we can look to bugs as a rough proxy of protocol complexity.
% % Generalized Paxos~\cite{lamport2005generalized} was published in 2005. Seven
% % years later, someone found a bug in one of its
% % assumptions~\cite{sutra2011fast}.  Zyzyva~\cite{kotla2007zyzzyva}, a Byzantine
% % replication protocol, was published in 2007. Ten years later, the authors
% % published a paper noting that the protocol is actually
% % unsafe~\cite{abraham2017revisiting}. In writing this paper, we discovered bugs
% % ourselves in two other protocols, EPaxos~\cite{moraru2013there} and
% % DPaxos~\cite{nawab2018dpaxos}, which we confirmed with the protocols' authors.
% % While these long undiscovered bugs are not \emph{hard evidence} that these
% % protocols are overly complex, it is suggestive.
%
% It is difficult to precisely measure the ``simplicity'' of a protocol. However,
% we can look at a couple of anecdotes that corroborate our claims of simplicity.
% For example, BPaxos' TLA+ specification (\appref{TlaSpec}) is 142 lines long. The TLA+
% specifications of two other state of the art protocols, EPaxos and Caesar, are
% 583 and 674 lines long respectively~\cite{moraru2013there, arun2017speeding}. While the length of a protocol's TLA+ specification is an imperfect metric of simplicity, the 4x reduction in length represents
% %
% % Second, the entire BPaxos protocol and all proofs required to understand it fit
% % in this paper. On the other hand, the full EPaxos protocol is deferred to a
% % technical report~\cite{moraru2013proof}, Caesar's proof of correctness is
% % placed in the appendix~\cite{arun2017speeding}, and the Generalized Paxos paper
% % is a whopping 35 pages long (43 pages including
% % proofs)~\cite{lamport2005generalized}.
% %
% As another example, EPaxos has a lengthy five page proof demonstrating that the
% protocol implements consensus correctly. We discovered (and confirmed with the
% authors) that there is a bug in the protocol and proof. BPaxos' proof, on the
% other hand, is zero lines long! We use Paxos unmodified and inherit its
% correctness without modification.
